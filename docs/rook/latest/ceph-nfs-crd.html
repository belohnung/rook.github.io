












































<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />

    
    <meta name="robots" content="noindex">
    

    <title>Ceph Docs</title>

    <link rel="canonical" href="https://rook.io/docs/rook/latest/ceph-nfs-crd.html">

    <link rel="icon" href="/favicon.ico" />
<link rel="icon" type="image/png" href="/images/favicon_16x16.png" sizes="16x16" />
<link rel="icon" type="image/png" href="/images/favicon_32x32.png" sizes="32x32" />
<link rel="icon" type="image/png" href="/images/favicon_48x48.png" sizes="48x48" />
<link rel="icon" type="image/png" href="/images/favicon_192x192.png" sizes="192x192" />


    <link href="//fonts.googleapis.com/css?family=Montserrat:500|Open+Sans:300,400,600" rel="stylesheet">
    
    <link rel="stylesheet" href="/css/main.css">
    
      <link rel="stylesheet" href="/css/docs.css" />
    
  </head>
  <body>
    <nav id="navigation" aria-label="Navigation">
  <div>
    <div class="logo">
      <a href="/"><img src="/images/rook-logo.svg"/></a>
    </div>
    <div
      class="hamburger-controls"
      onclick="if (document.body.classList.contains('menu-open')) { document.body.classList.remove('menu-open') } else { document.body.classList.add('menu-open') }; return false;">
      <span></span> <span></span> <span></span>
    </div>
    <ul class="links">
      <li class="dropdown">
        <a role="button" href="javascript:void(0)">Documentation</a>
        <div class="dropdown-content">
          <a href="/docs/rook/v1.9/">Ceph</a>
          <a href="/docs/cassandra/v1.7/">Cassandra</a>
          <a href="/docs/nfs/v1.7/">NFS</a>
        </div>
      <li class="dropdown">
        <a role="button" href="javascript:void(0)">Community</a>
        <div class="dropdown-content">
          <a href="//github.com/rook/rook">GitHub</a>
          <a href="//slack.rook.io/">Slack</a>
          <a href="//groups.google.com/forum/#!forum/rook-dev">Forum</a>
          <a href="//twitter.com/rook_io">Twitter</a>
        </div>
      </li>
      <li><a href="//blog.rook.io/">Blog</a></li>
      <li><a class="button small" href="/docs/rook/v1.9/quickstart.html">Get Started</a></li>
    </ul>
  </div>
</nav>

    <main id="content" aria-label="Content"><div>



















<section class="docs-header">
  <h1>Ceph</h1>
  <div class="versions">
    <a role="button" href="javascript:void(0)">Rook Ceph latest</a>
    <div class="versions-dropdown-content">
      
        <a href="/docs/rook/v1.9/ceph-nfs-crd.html">Rook Ceph v1.9</a>
      
        <a href="/docs/rook/v1.8/ceph-nfs-crd.html">Rook Ceph v1.8</a>
      
        <a href="/docs/rook/v1.7/ceph-nfs-crd.html">Rook Ceph v1.7</a>
      
        <a href="/docs/rook/v1.6/ceph-nfs-crd.html">Rook Ceph v1.6</a>
      
        <a href="/docs/rook/v1.5/ceph-nfs-crd.html">Rook Ceph v1.5</a>
      
        <a href="/docs/rook/v1.4/ceph-nfs-crd.html">Rook Ceph v1.4</a>
      
        <a href="/docs/rook/v1.3/ceph-nfs-crd.html">Rook Ceph v1.3</a>
      
        <a href="/docs/rook/v1.2/ceph-nfs-crd.html">Rook Ceph v1.2</a>
      
        <a href="/docs/rook/v1.1/ceph-nfs-crd.html">Rook Ceph v1.1</a>
      
        <a href="/docs/rook/v1.0/ceph-nfs-crd.html">Rook Ceph v1.0</a>
      
        <a href="/docs/rook/v0.9/ceph-nfs-crd.html">Rook Ceph v0.9</a>
      
        <a href="/docs/rook/v0.8/ceph-nfs-crd.html">Rook Ceph v0.8</a>
      
        <a href="/docs/rook/v0.7/ceph-nfs-crd.html">Rook Ceph v0.7</a>
      
        <a href="/docs/rook/v0.6/ceph-nfs-crd.html">Rook Ceph v0.6</a>
      
        <a href="/docs/rook/v0.5/ceph-nfs-crd.html">Rook Ceph v0.5</a>
      
        <a href="/docs/rook/latest/ceph-nfs-crd.html" class="active">Rook Ceph latest</a>
      
    </div>
    <img src="/images/arrow.svg" />
  </div>
</section>
<div class="page">
  <div class="docs-menu">
      <ul id="docs-ul"></ul>
  </div>
  <div class="docs-content">
    <div class="docs-actions">
      <a id="edit" href="https://github.com/rook/rook/blob/master/Documentation/ceph-nfs-crd.md">Edit on GitHub</a>
    </div>
    
      <div class="alert master">
        <p><b>PLEASE NOTE:</b> This document applies to an <b>unreleased</b> version of Rook. It is strongly recommended that you only use official releases of Rook, as unreleased versions are subject to changes and incompatibilities that will not be supported in the official releases.</p>
      </div>
    
    <div class="docs-text">
      
<h1 id="ceph-nfs-server-crd">Ceph NFS Server CRD</h1>

<h2 id="overview">Overview</h2>
<p>Rook allows exporting NFS shares of a CephFilesystem or CephObjectStore through the CephNFS custom
resource definition. This will spin up a cluster of
<a href="https://github.com/nfs-ganesha/nfs-ganesha">NFS Ganesha</a> servers that coordinate with one another
via shared RADOS objects. The servers will be configured for NFSv4.1+ access only, as serving
earlier protocols can inhibit responsiveness after a server restart.</p>

<blockquote>
  <p><strong>WARNING</strong>: Due to a number of Ceph issues and changes, Rook officially only supports Ceph
v16.2.7 or higher for CephNFS. If you are using an earlier version, upgrade your Ceph version
following the advice given in Rookâ€™s
<a href="https://rook.github.io/docs/rook/v1.8/ceph-nfs-crd.html">v1.8 NFS docs</a>.</p>
</blockquote>

<h2 id="samples">Samples</h2>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">ceph.rook.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">CephNFS</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-nfs</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">rook-ceph</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="c1"># Settings for the NFS server</span>
  <span class="na">server</span><span class="pi">:</span>
    <span class="na">active</span><span class="pi">:</span> <span class="m">1</span>

    <span class="na">placement</span><span class="pi">:</span>
      <span class="na">nodeAffinity</span><span class="pi">:</span>
        <span class="na">requiredDuringSchedulingIgnoredDuringExecution</span><span class="pi">:</span>
          <span class="na">nodeSelectorTerms</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">matchExpressions</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">role</span>
              <span class="na">operator</span><span class="pi">:</span> <span class="s">In</span>
              <span class="na">values</span><span class="pi">:</span>
              <span class="pi">-</span> <span class="s">nfs-node</span>
      <span class="na">topologySpreadConstraints</span><span class="pi">:</span>
      <span class="na">tolerations</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">nfs-node</span>
        <span class="na">operator</span><span class="pi">:</span> <span class="s">Exists</span>
      <span class="na">podAffinity</span><span class="pi">:</span>
      <span class="na">podAntiAffinity</span><span class="pi">:</span>

    <span class="na">annotations</span><span class="pi">:</span>
      <span class="na">my-annotation</span><span class="pi">:</span> <span class="s">something</span>

    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">my-label</span><span class="pi">:</span> <span class="s">something</span>

    <span class="na">resources</span><span class="pi">:</span>
      <span class="na">limits</span><span class="pi">:</span>
        <span class="na">cpu</span><span class="pi">:</span> <span class="s2">"</span><span class="s">500m"</span>
        <span class="na">memory</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1024Mi"</span>
      <span class="na">requests</span><span class="pi">:</span>
        <span class="na">cpu</span><span class="pi">:</span> <span class="s2">"</span><span class="s">500m"</span>
        <span class="na">memory</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1024Mi"</span>

    <span class="na">priorityClassName</span><span class="pi">:</span>

    <span class="na">logLevel</span><span class="pi">:</span> <span class="s">NIV_INFO</span>
</code></pre></div></div>

<h2 id="nfs-settings">NFS Settings</h2>

<h3 id="server">Server</h3>
<p>The <code class="language-plaintext highlighter-rouge">server</code> spec sets configuration for Rook-created NFS-Ganesha servers.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">active</code>: The number of active NFS servers. Rook supports creating more than one active NFS
server, but cannot guarantee high availability. For values greater than 1, see the
<a href="#serveractive-count-greater-than-1">known issue</a> below.</li>
  <li><code class="language-plaintext highlighter-rouge">placement</code>: Kubernetes placement restrictions to apply to NFS server Pod(s). This is similar to
placement defined for daemons configured by the
<a href="https://github.com/rook/rook/blob/master/deploy/examples/cluster.yaml">CephCluster CRD</a>.</li>
  <li><code class="language-plaintext highlighter-rouge">annotations</code>: Kubernetes annotations to apply to NFS server Pod(s)</li>
  <li><code class="language-plaintext highlighter-rouge">labels</code>: Kubernetes labels to apply to NFS server Pod(s)</li>
  <li><code class="language-plaintext highlighter-rouge">resources</code>: Kubernetes resource requests and limits to set on NFS server Pod(s)</li>
  <li><code class="language-plaintext highlighter-rouge">priorityClassName</code>: Set priority class name for the NFS server Pod(s)</li>
  <li><code class="language-plaintext highlighter-rouge">logLevel</code>: The log level that NFS-Ganesha servers should output.&lt;/br&gt;
Default value: NIV_INFO&lt;/br&gt;
Supported values: NIV_NULL | NIV_FATAL | NIV_MAJ | NIV_CRIT | NIV_WARN | NIV_EVENT | NIV_INFO | NIV_DEBUG | NIV_MID_DEBUG | NIV_FULL_DEBUG | NB_LOG_LEVEL</li>
</ul>

<h2 id="creating-exports">Creating Exports</h2>
<p>When a CephNFS is first created, all NFS daemons within the CephNFS cluster will share a
configuration with no exports defined.</p>

<h3 id="using-the-ceph-dashboard">Using the Ceph Dashboard</h3>
<p>Exports can be created via the
<a href="https://docs.ceph.com/en/latest/mgr/dashboard/#nfs-ganesha-management">Ceph dashboard</a> for Ceph v16
as well. To enable and use the Ceph dashboard in Rook, see <a href="/docs/rook/latest/ceph-dashboard.html">here</a>.</p>

<h3 id="using-the-ceph-cli">Using the Ceph CLI</h3>
<p>The Ceph CLI can be used from the Rook toolbox pod to create and manage NFS exports. To do so, first
ensure the necessary Ceph mgr modules are enabled, if necessary, and that the Ceph orchestrator
backend is set to Rook.</p>

<blockquote>
  <h4 id="enable-the-ceph-orchestrator-if-necessary"><strong>Enable the Ceph orchestrator if necessary</strong></h4>
  <ul>
    <li>Required for Ceph v16.2.7 and below</li>
    <li>Optional for Ceph v16.2.8 and above</li>
    <li>Must be disabled for Ceph v17.2.0 due to a <a href="#ceph-v1720">Ceph regression</a>
      <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">ceph mgr module enable rook
ceph mgr module enable nfs
ceph orch set backend rook
</span></code></pre></div>      </div>
    </li>
  </ul>
</blockquote>

<p><a href="https://docs.ceph.com/en/latest/mgr/nfs/#export-management">Cephâ€™s NFS CLI</a> can create NFS exports
that are backed by <a href="https://docs.ceph.com/en/latest/cephfs/nfs/">CephFS</a> (a CephFilesystem) or
<a href="https://docs.ceph.com/en/latest/radosgw/nfs/">Ceph Object Gateway</a> (a CephObjectStore).
<code class="language-plaintext highlighter-rouge">cluster_id</code> or <code class="language-plaintext highlighter-rouge">cluster-name</code> in the Ceph NFS docs normally refers to the name of the NFS cluster,
which is the CephNFS name in the Rook context.</p>

<p>For creating an NFS export for the CephNFS and CephFilesystem example manifests, the below command
can be used. This creates an export for the <code class="language-plaintext highlighter-rouge">/test</code> pseudo path.</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">ceph nfs export create cephfs my-nfs /test myfs
</span></code></pre></div></div>

<p>The below command will list the current NFS exports for the example CephNFS cluster, which will give
the output shown for the current example.</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">ceph nfs export ls my-nfs
</span></code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[
  "/test"
]
</code></pre></div></div>

<p>The simple <code class="language-plaintext highlighter-rouge">/test</code> exportâ€™s info can be listed as well. Notice from the example that only NFS
protocol v4 via TCP is supported.</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">ceph nfs export info my-nfs /test
</span></code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
  "export_id": 1,
  "path": "/",
  "cluster_id": "my-nfs",
  "pseudo": "/test",
  "access_type": "RW",
  "squash": "none",
  "security_label": true,
  "protocols": [
    4
  ],
  "transports": [
    "TCP"
  ],
  "fsal": {
    "name": "CEPH",
    "user_id": "nfs.my-nfs.1",
    "fs_name": "myfs"
  },
  "clients": []
}
</code></pre></div></div>

<p>If you are done managing NFS exports and donâ€™t need the Ceph orchestrator module enabled for
anything else, it may be preferable to disable the Rook and NFS mgr modules to free up a small
amount of RAM in the Ceph mgr Pod.</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">ceph orch set backend ""
ceph mgr module disable rook
</span></code></pre></div></div>

<h3 id="mounting-exports">Mounting exports</h3>
<p>Each CephNFS server has a unique Kubernetes Service. This is because NFS clients canâ€™t readily
handle NFS failover. CephNFS services are named with the pattern
<code class="language-plaintext highlighter-rouge">rook-ceph-nfs-&lt;cephnfs-name&gt;-&lt;id&gt;</code> <code class="language-plaintext highlighter-rouge">&lt;id&gt;</code> is a unique letter ID (e.g., a, b, c, etc.) for a given
NFS server. For example, <code class="language-plaintext highlighter-rouge">rook-ceph-nfs-my-nfs-a</code>.</p>

<p>For each NFS client, choose an NFS service to use for the connection. With NFS v4, you can mount an
export by its path using a mount command like below. You can mount all exports at once by omitting
the export path and leaving the directory as just <code class="language-plaintext highlighter-rouge">/</code>.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mount -t nfs4 -o proto=tcp &lt;nfs-service-address&gt;:/&lt;export-path&gt; &lt;mount-location&gt;
</code></pre></div></div>

<h2 id="exposing-the-nfs-server-outside-of-the-kubernetes-cluster">Exposing the NFS server outside of the Kubernetes cluster</h2>
<p>Use a LoadBalancer Service to expose an NFS server (and its exports) outside of the Kubernetes
cluster. The Serviceâ€™s endpoint can be used as the NFS service address when
<a href="#mounting-exports">mounting the export manually</a>. We provide an example Service here:
<a href="https://github.com/rook/rook/tree/master/deploy/examples"><code class="language-plaintext highlighter-rouge">deploy/examples/nfs-load-balancer.yaml</code></a>.</p>

<h2 id="scaling-the-active-server-count">Scaling the active server count</h2>
<p>It is possible to scale the size of the cluster up or down by modifying the <code class="language-plaintext highlighter-rouge">spec.server.active</code>
field. Scaling the cluster size up can be done at will. Once the new server comes up, clients can be
assigned to it immediately.</p>

<p>The CRD always eliminates the highest index servers first, in reverse order from how they were
started. Scaling down the cluster requires that clients be migrated from servers that will be
eliminated to others. That process is currently a manual one and should be performed before reducing
the size of the cluster.</p>

<blockquote>
  <p><strong>WARNING:</strong> see the <a href="#serveractive-count-greater-than-1">known issue</a> below about setting this
value greater than one.</p>
</blockquote>

<h2 id="known-issues">Known issues</h2>
<h3 id="serveractive-count-greater-than-1">server.active count greater than 1</h3>
<ul>
  <li>Active-active scale out does not work well with the NFS protocol. If one NFS server in a cluster
is offline, other servers may block client requests until the offline server returns, which may
not always happen due to the Kubernetes scheduler.
    <ul>
      <li>Workaround: It is safest to run only a single NFS server, but we do not limit this if it
benefits your use case.</li>
    </ul>
  </li>
</ul>

<h3 id="ceph-v1720">Ceph v17.2.0</h3>
<ul>
  <li>Ceph NFS management with the Rook mgr module enabled has a breaking regression with the Ceph
Quincy v17.2.0 release.
    <ul>
      <li>Workaround: Leave Cephâ€™s Rook orchestrator mgr module disabled. If you have enabled it, you must
disable it using the snippet below from the toolbox.
        <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">ceph orch set backend ""
ceph mgr module disable rook
</span></code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<h2 id="advanced-configuration">Advanced configuration</h2>
<p>All CephNFS daemons are configured using shared RADOS objects stored in a Ceph pool named <code class="language-plaintext highlighter-rouge">.nfs</code>.
Users can modify the configuration object for each CephNFS cluster if they wish to customize the
configuration.</p>

<h3 id="changing-configuration-of-the-nfs-pool">Changing configuration of the .nfs pool</h3>
<p>By default, Rook creates the <code class="language-plaintext highlighter-rouge">.nfs</code> pool with Cephâ€™s default configuration. If you wish to change
the configuration of this pool (for example to change its failure domain or replication factor), you
can create a CephBlockPool with the <code class="language-plaintext highlighter-rouge">spec.name</code> field set to <code class="language-plaintext highlighter-rouge">.nfs</code>. This pool <strong>must</strong> be
replicated and <strong>cannot</strong> be erasure coded.
<a href="https://github.com/rook/rook/blob/master/deploy/examples/nfs.yaml"><code class="language-plaintext highlighter-rouge">deploy/examples/nfs.yaml</code></a>
contains a sample for reference.</p>

<h3 id="adding-custom-nfs-ganesha-config-file-changes">Adding custom NFS-Ganesha config file changes</h3>
<p>The NFS-Ganesha config file format for these objects is documented in the
<a href="https://github.com/nfs-ganesha/nfs-ganesha/wiki">NFS-Ganesha project</a>.</p>

<p>Use Cephâ€™s <code class="language-plaintext highlighter-rouge">rados</code> tool from the toolbox to interact with the configuration object. The below
command will get you started by dumping the contents of the config object to stdout. The output will
look something like the example shown if you have already created two exports as documented above.
It is best not to modify any of the export objects created by Ceph so as not to cause errors with
Cephâ€™s export management.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">rados --pool &lt;pool&gt;</span><span class="w"> </span><span class="nt">--namespace</span> &lt;namespace&gt; get conf-nfs.&lt;cephnfs-name&gt; -
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>%url "rados://&lt;pool&gt;/&lt;namespace&gt;/export-1"
%url "rados://&lt;pool&gt;/&lt;namespace&gt;/export-2"
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">rados ls</code> and <code class="language-plaintext highlighter-rouge">rados put</code> are other commands you will want to work with the other shared
configuration objects.</p>

<p>Of note, it is possible to pre-populate the NFS configuration and export objects prior to creating
CephNFS server clusters.</p>

<h2 id="ceph-csi-nfs-provisioner-and-nfs-csi-driver">Ceph CSI NFS provisioner and NFS CSI driver</h2>
<blockquote>
  <p><strong>EXPERIMENTAL</strong>: this feature is experimental, and we do not guarantee it is bug-free, nor will
we support upgrades to future versions</p>
</blockquote>

<p>In version 1.9.1, Rook is able to deploy the experimental NFS Ceph CSI driver. This requires Ceph
CSI version 3.6.0 or above. We recommend Ceph v16.2.7 or above.</p>

<p>For this section, we will refer to Rookâ€™s deployment examples in the
<a href="https://github.com/rook/rook/tree/master/deploy/examples">deploy/examples</a> directory.</p>

<p>The Ceph CSI NFS provisioner and driver require additional RBAC to operate. Apply the
<code class="language-plaintext highlighter-rouge">deploy/examples/csi/nfs/rbac.yaml</code> manifest to deploy the additional resources.</p>

<p>Rook will only deploy the Ceph CSI NFS provisioner and driver components when the
<code class="language-plaintext highlighter-rouge">ROOK_CSI_ENABLE_NFS</code> config is set to <code class="language-plaintext highlighter-rouge">"true"</code> in the <code class="language-plaintext highlighter-rouge">rook-ceph-operator-config</code> configmap. Change
the value in your manifest, or patch the resource as below.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl <span class="nt">--namespace</span> rook-ceph patch configmap rook-ceph-operator-config <span class="nt">--type</span> merge <span class="nt">--patch</span> <span class="s1">'{"data":{"ROOK_CSI_ENABLE_NFS": "true"}}'</span>
</code></pre></div></div>

<blockquote>
  <p><strong>NOTE:</strong> The rook-ceph operator Helm chart will deploy the required RBAC and enable the driver
components if <code class="language-plaintext highlighter-rouge">csi.nfs.enabled</code> is set to <code class="language-plaintext highlighter-rouge">true</code>.</p>
</blockquote>

<p>In order to create NFS exports via the CSI driver, you must first create a CephFilesystem to serve
as the underlying storage for the exports, and you must create a CephNFS to run an NFS server that
will expose the exports.</p>

<p>From the examples, <code class="language-plaintext highlighter-rouge">filesystem.yaml</code> creates a CephFilesystem called <code class="language-plaintext highlighter-rouge">myfs</code>, and <code class="language-plaintext highlighter-rouge">nfs.yaml</code> creates
an NFS server called <code class="language-plaintext highlighter-rouge">my-nfs</code>.</p>

<p>You may need to enable or disable the Ceph orchestrator. Follow the same steps documented
<a href="#enable-the-ceph-orchestrator-if-necessary">above</a> based on your Ceph version and desires.</p>

<p>You must also create a storage class. Ceph CSI is designed to support any arbitrary Ceph cluster,
but we are focused here only on Ceph clusters deployed by Rook. Letâ€™s take a look at a portion of
the example storage class found at <code class="language-plaintext highlighter-rouge">deploy/examples/csi/nfs/storageclass.yaml</code> and break down how
the values are determined.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">storage.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">StorageClass</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">rook-nfs</span>
<span class="na">provisioner</span><span class="pi">:</span> <span class="s">rook-ceph.nfs.csi.ceph.com</span> <span class="c1"># [1]</span>
<span class="na">parameters</span><span class="pi">:</span>
  <span class="na">nfsCluster</span><span class="pi">:</span> <span class="s">my-nfs</span> <span class="c1"># [2]</span>
  <span class="na">server</span><span class="pi">:</span> <span class="s">rook-ceph-nfs-my-nfs-a</span> <span class="c1"># [3]</span>
  <span class="na">clusterID</span><span class="pi">:</span> <span class="s">rook-ceph</span> <span class="c1"># [4]</span>
  <span class="na">fsName</span><span class="pi">:</span> <span class="s">myfs</span> <span class="c1"># [5]</span>
  <span class="na">pool</span><span class="pi">:</span> <span class="s">myfs-replicated</span> <span class="c1"># [6]</span>

  <span class="c1"># [7] (entire csi.storage.k8s.io/* section immediately below)</span>
  <span class="s">csi.storage.k8s.io/provisioner-secret-name</span><span class="pi">:</span> <span class="s">rook-csi-cephfs-provisioner</span>
  <span class="s">csi.storage.k8s.io/provisioner-secret-namespace</span><span class="pi">:</span> <span class="s">rook-ceph</span>
  <span class="s">csi.storage.k8s.io/controller-expand-secret-name</span><span class="pi">:</span> <span class="s">rook-csi-cephfs-provisioner</span>
  <span class="s">csi.storage.k8s.io/controller-expand-secret-namespace</span><span class="pi">:</span> <span class="s">rook-ceph</span>
  <span class="s">csi.storage.k8s.io/node-stage-secret-name</span><span class="pi">:</span> <span class="s">rook-csi-cephfs-node</span>
  <span class="s">csi.storage.k8s.io/node-stage-secret-namespace</span><span class="pi">:</span> <span class="s">rook-ceph</span>

<span class="c1"># ... some fields omitted ...</span>
</code></pre></div></div>

<ol>
  <li><code class="language-plaintext highlighter-rouge">provisioner</code>: <strong>rook-ceph</strong>.nfs.csi.ceph.com because <strong>rook-ceph</strong> is the namespace where the
CephCluster is installed</li>
  <li><code class="language-plaintext highlighter-rouge">nfsCluster</code>: <strong>my-nfs</strong> because this is the name of the CephNFS</li>
  <li><code class="language-plaintext highlighter-rouge">server</code>: rook-ceph-nfs-<strong>my-nfs</strong>-a because Rook creates this Kubernetes Service for the CephNFS
named <strong>my-nfs</strong></li>
  <li><code class="language-plaintext highlighter-rouge">clusterID</code>: <strong>rook-ceph</strong> because this is the namespace where the CephCluster is installed</li>
  <li><code class="language-plaintext highlighter-rouge">fsName</code>: <strong>myfs</strong> because this is the name of the CephFilesystem used to back the NFS exports</li>
  <li><code class="language-plaintext highlighter-rouge">pool</code>: <strong>myfs</strong>-<strong>replicated</strong> because <strong>myfs</strong> is the name of the CephFilesystem defined in
<code class="language-plaintext highlighter-rouge">fsName</code> and because <strong>replicated</strong> is the name of a data pool defined in the CephFilesystem</li>
  <li><code class="language-plaintext highlighter-rouge">csi.storage.k8s.io/*</code>: note that these values are shared with the Ceph CSI CephFS provisioner</li>
</ol>

<p>See <code class="language-plaintext highlighter-rouge">deploy/examples/csi/nfs/pvc.yaml</code> for an example of how to create a PVC that will create an NFS
export. The export will be created and a PV created for the PVC immediately, even without a Pod to
mount the PVC. The <code class="language-plaintext highlighter-rouge">share</code> parameter set on the resulting PV contains the share path (<code class="language-plaintext highlighter-rouge">share</code>) which
can be used as the export path when <a href="#mounting-exports">mounting the export manually</a>.</p>

<p>See <code class="language-plaintext highlighter-rouge">deploy/examples/csi/nfs/pod.yaml</code> for an example of how a PVC can be connected to an
application pod.</p>

    </div>
  </div>
</div>

<script>
  var menu = [];
  var BASE_PATH = "";

  function add(name, url, isChild, current) {
    var item = { name: name, url: url, current: current };
    var container = menu;
    if (isChild && menu.length > 0) {
      menu[menu.length-1].children = menu[menu.length-1].children || [];
      container = menu[menu.length-1].children;
      if (current) {
        menu[menu.length-1].childCurrent = true;
      }
    }
    container.push(item);
  }

  
    add(
      "Rook",
      "/docs/rook/latest/",
      false,
      false
    );
  
    add(
      "Quickstart",
      "/docs/rook/latest/quickstart.html",
      false,
      false
    );
  
    add(
      "Prerequisites",
      "/docs/rook/latest/pre-reqs.html",
      false,
      false
    );
  
    add(
      "Authenticated Registries",
      "/docs/rook/latest/authenticated-registry.html",
      true,
      false
    );
  
    add(
      "Pod Security Policies",
      "/docs/rook/latest/pod-security-policies.html",
      true,
      false
    );
  
    add(
      "Ceph Storage",
      "/docs/rook/latest/ceph-storage.html",
      false,
      false
    );
  
    add(
      "Examples",
      "/docs/rook/latest/ceph-examples.html",
      true,
      false
    );
  
    add(
      "OpenShift",
      "/docs/rook/latest/ceph-openshift.html",
      true,
      false
    );
  
    add(
      "Block Storage",
      "/docs/rook/latest/ceph-block.html",
      true,
      false
    );
  
    add(
      "Object Storage",
      "/docs/rook/latest/ceph-object.html",
      true,
      false
    );
  
    add(
      "Object Multisite",
      "/docs/rook/latest/ceph-object-multisite.html",
      true,
      false
    );
  
    add(
      "Shared Filesystem",
      "/docs/rook/latest/ceph-filesystem.html",
      true,
      false
    );
  
    add(
      "Ceph Dashboard",
      "/docs/rook/latest/ceph-dashboard.html",
      true,
      false
    );
  
    add(
      "Prometheus Monitoring",
      "/docs/rook/latest/ceph-monitoring.html",
      true,
      false
    );
  
    add(
      "Cluster CRD",
      "/docs/rook/latest/ceph-cluster-crd.html",
      true,
      false
    );
  
    add(
      "Block Pool CRD",
      "/docs/rook/latest/ceph-pool-crd.html",
      true,
      false
    );
  
    add(
      "Object Store CRD",
      "/docs/rook/latest/ceph-object-store-crd.html",
      true,
      false
    );
  
    add(
      "Object Multisite CRDs",
      "/docs/rook/latest/ceph-object-multisite-crd.html",
      true,
      false
    );
  
    add(
      "Object Bucket Claim",
      "/docs/rook/latest/ceph-object-bucket-claim.html",
      true,
      false
    );
  
    add(
      "Object Store User CRD",
      "/docs/rook/latest/ceph-object-store-user-crd.html",
      true,
      false
    );
  
    add(
      "Bucket Notifications",
      "/docs/rook/latest/ceph-object-bucket-notifications.html",
      true,
      false
    );
  
    add(
      "Shared Filesystem CRD",
      "/docs/rook/latest/ceph-filesystem-crd.html",
      true,
      false
    );
  
    add(
      "NFS CRD",
      "/docs/rook/latest/ceph-nfs-crd.html",
      true,
      true
    );
  
    add(
      "Ceph CSI",
      "/docs/rook/latest/ceph-csi-drivers.html",
      true,
      false
    );
  
    add(
      "RBD Mirroring",
      "/docs/rook/latest/rbd-mirroring.html",
      true,
      false
    );
  
    add(
      "Failover and Failback",
      "/docs/rook/latest/async-disaster-recovery.html",
      true,
      false
    );
  
    add(
      "Snapshots",
      "/docs/rook/latest/ceph-csi-snapshot.html",
      true,
      false
    );
  
    add(
      "Volume clone",
      "/docs/rook/latest/ceph-csi-volume-clone.html",
      true,
      false
    );
  
    add(
      "Client CRD",
      "/docs/rook/latest/ceph-client-crd.html",
      true,
      false
    );
  
    add(
      "RBD Mirror CRD",
      "/docs/rook/latest/ceph-rbd-mirror-crd.html",
      true,
      false
    );
  
    add(
      "Filesystem Mirror CRD",
      "/docs/rook/latest/ceph-fs-mirror-crd.html",
      true,
      false
    );
  
    add(
      "SubVolume Group CRD",
      "/docs/rook/latest/ceph-fs-subvolumegroup.html",
      true,
      false
    );
  
    add(
      "RADOS Namespace CRD",
      "/docs/rook/latest/ceph-pool-radosnamespace.html",
      true,
      false
    );
  
    add(
      "Key Management System",
      "/docs/rook/latest/ceph-kms.html",
      true,
      false
    );
  
    add(
      "Configuration",
      "/docs/rook/latest/ceph-configuration.html",
      true,
      false
    );
  
    add(
      "Upgrades",
      "/docs/rook/latest/ceph-upgrade.html",
      true,
      false
    );
  
    add(
      "Cleanup",
      "/docs/rook/latest/ceph-teardown.html",
      true,
      false
    );
  
    add(
      "Helm Charts",
      "/docs/rook/latest/helm.html",
      false,
      false
    );
  
    add(
      "Ceph Operator",
      "/docs/rook/latest/helm-operator.html",
      true,
      false
    );
  
    add(
      "Ceph Cluster",
      "/docs/rook/latest/helm-ceph-cluster.html",
      true,
      false
    );
  
    add(
      "Common Issues",
      "/docs/rook/latest/common-issues.html",
      false,
      false
    );
  
    add(
      "Ceph Tools",
      "/docs/rook/latest/ceph-tools.html",
      false,
      false
    );
  
    add(
      "Toolbox",
      "/docs/rook/latest/ceph-toolbox.html",
      true,
      false
    );
  
    add(
      "Common Issues",
      "/docs/rook/latest/ceph-common-issues.html",
      true,
      false
    );
  
    add(
      "CSI Common Issues",
      "/docs/rook/latest/ceph-csi-troubleshooting.html",
      true,
      false
    );
  
    add(
      "Monitor Health",
      "/docs/rook/latest/ceph-mon-health.html",
      true,
      false
    );
  
    add(
      "OSD Management",
      "/docs/rook/latest/ceph-osd-mgmt.html",
      true,
      false
    );
  
    add(
      "Direct Tools",
      "/docs/rook/latest/direct-tools.html",
      true,
      false
    );
  
    add(
      "Advanced Configuration",
      "/docs/rook/latest/ceph-advanced-configuration.html",
      true,
      false
    );
  
    add(
      "OpenShift Common Issues",
      "/docs/rook/latest/ceph-openshift-issues.html",
      true,
      false
    );
  
    add(
      "Disaster Recovery",
      "/docs/rook/latest/ceph-disaster-recovery.html",
      true,
      false
    );
  
    add(
      "Contributing",
      "/docs/rook/latest/development-flow.html",
      false,
      false
    );
  
    add(
      "Developer Environment",
      "/docs/rook/latest/development-environment.html",
      true,
      false
    );
  
    add(
      "Storage Providers",
      "/docs/rook/latest/storage-providers.html",
      true,
      false
    );
  

  function getEntry(item) {
    var itemDom = document.createElement('li');

    if (item.current) {
      itemDom.innerHTML = item.name;
      itemDom.classList.add('current');
    } else {
      itemDom.innerHTML = '<a href="' + item.url + '">' + item.name + '</a>';
    }

    return itemDom;
  }

  // Flush css changes as explained in: https://stackoverflow.com/a/34726346
  // and more completely: https://stackoverflow.com/a/6956049
  function flushCss(element) {
    element.offsetHeight;
  }

  function addArrow(itemDom) {
    var MAIN_ITEM_HEIGHT = 24;
    var BOTTOM_PADDING = 20;
    var arrowDom = document.createElement('a');
    arrowDom.classList.add('arrow');
    arrowDom.innerHTML = '<img src="' + BASE_PATH + '/images/arrow.svg" />';
    arrowDom.onclick = function(itemDom) {
      return function () {
        // Calculated full height of the opened list
        var fullHeight = MAIN_ITEM_HEIGHT + BOTTOM_PADDING + itemDom.lastChild.clientHeight + 'px';

        itemDom.classList.toggle('open');

        if (itemDom.classList.contains('open')) {
          itemDom.style.height = fullHeight;
        } else {
          // If the list height is auto we have to set it to fullHeight
          // without tranistion before we shrink it to collapsed height
          if (itemDom.style.height === 'auto') {
            itemDom.style.transition = 'none';
            itemDom.style.height = fullHeight;
            flushCss(itemDom);
            itemDom.style.transition = '';
          }
          itemDom.style.height = MAIN_ITEM_HEIGHT + 'px';
        }

        return false;
      };
    }(itemDom);
    itemDom.appendChild(arrowDom);

    if ((item.current && item.children) || item.childCurrent) {
      itemDom.classList.add('open');
      itemDom.style.height = 'auto';
    } else {
      itemDom.style.height = MAIN_ITEM_HEIGHT + 'px';
    }
  }

  var menuDom = document.getElementById('docs-ul');
  for (var i = 0; i < menu.length; i++) {
    var item = menu[i];
    var itemDom = getEntry(item);

    if (item.childCurrent) {
      itemDom.classList.add('childCurrent');
    }

    if (item.children) {
      addArrow(itemDom);
      itemDom.classList.add('children');
      var children = document.createElement('ul');
      for (var j = 0; j < item.children.length; j++) {
        children.appendChild(getEntry(item.children[j]));
      }
      itemDom.appendChild(children);
    }
    menuDom.appendChild(itemDom);
  }
</script>
</div></main>
    <footer id="footer" aria-label="Footer">
  <div class="top">
    <a href="//www.cncf.io">
      <img
        class="cncf"
        src="/images/cncf.png"
        srcset="/images/cncf@2x.png 2x, /images/cncf@3x.png 3x" />
    </a>
    <p>We are a Cloud Native Computing Foundation graduated project.</p>
  </div>
  <div class="middle">
    <div class="grid-center">
      <div class="col_sm-12">
        <span>Getting Started</span>
        <a href="//github.com/rook/rook">GitHub</a>
        <a href="/docs/rook/v1.9/">Documentation</a>
        <a href="//github.com/rook/rook/blob/master/CONTRIBUTING.md#how-to-contribute">How to Contribute</a>
      </div>
      <div class="col_sm-12">
        <span>Community</span>
        <a href="//slack.rook.io/">Slack</a>
        <a href="//twitter.com/rook_io">Twitter</a>
        <a href="//groups.google.com/forum/#!forum/rook-dev">Forum</a>
        <a href="//blog.rook.io/">Blog</a>
      </div>
      <div class="col_sm-12">
        <span>Contact</span>
        <a href="mailto:cncf-rook-info@lists.cncf.io">Email</a>
        <a href="//github.com/rook/rook/issues">Feature request</a>
      </div>
      <div class="col_sm-12">
        <span>Top Contributors</span>
        <a href="//cloudical.io/">Cloudical</a>
        <a href="//cybozu.com">Cybozu, Inc</a>
        <a href="//www.redhat.com">Red Hat</a>
        <a href="//www.suse.com/">SUSE</a>
        <a href="//upbound.io">Upbound</a>
      </div>
    </div>
  </div>
  <div class="bottom">
    <div class="grid-center">
      <div class="col-8">
        <a class="logo" href="/">
          <img src="/images/rook-logo-small.svg" alt="rook.io" />
        </a>
        <p>
          &#169; Rook Authors 2022. Documentation distributed under
          <a href="https://creativecommons.org/licenses/by/4.0">CC-BY-4.0</a>.
        </p>
        <p>
          &#169; 2022 The Linux Foundation. All rights reserved. The Linux Foundation has
          registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our
          <a href="https://www.linuxfoundation.org/trademark-usage/">Trademark Usage</a> page.
        </p>
      </div>
    </div>
  </div>
</footer>


  <script src="/js/anchor.js"></script>
  <script>
    anchors.options = {
      placement: 'right',
      icon: '#',
    }

    document.addEventListener('DOMContentLoaded', function(event) {
      anchors.add('.docs-text h1, .docs-text h2, .docs-text h3, .docs-text h4, .docs-text h5, .docs-text h6');
    });
  </script>




    
  </body>
</html>
