












































<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />

    
    <meta name="robots" content="noindex">
    

    <title>Ceph Docs</title>

    <link rel="canonical" href="https://rook.io/docs/rook/v1.5/ceph-common-issues.html">

    <link rel="icon" href="/favicon.ico" />
<link rel="icon" type="image/png" href="/images/favicon_16x16.png" sizes="16x16" />
<link rel="icon" type="image/png" href="/images/favicon_32x32.png" sizes="32x32" />
<link rel="icon" type="image/png" href="/images/favicon_48x48.png" sizes="48x48" />
<link rel="icon" type="image/png" href="/images/favicon_192x192.png" sizes="192x192" />


    <link href="//fonts.googleapis.com/css?family=Montserrat:500|Open+Sans:300,400,600" rel="stylesheet">
    
    <link rel="stylesheet" href="/css/main.css">
    
      <link rel="stylesheet" href="/css/docs.css" />
    
  </head>
  <body>
    <nav id="navigation" aria-label="Navigation">
  <div>
    <div class="logo">
      <a href="/"><img src="/images/rook-logo.svg"/></a>
    </div>
    <div
      class="hamburger-controls"
      onclick="if (document.body.classList.contains('menu-open')) { document.body.classList.remove('menu-open') } else { document.body.classList.add('menu-open') }; return false;">
      <span></span> <span></span> <span></span>
    </div>
    <ul class="links">
      <li class="dropdown">
        <a role="button" href="javascript:void(0)">Documentation</a>
        <div class="dropdown-content">
          <a href="/docs/rook/v1.9/">Ceph</a>
          <a href="/docs/cassandra/v1.7/">Cassandra</a>
          <a href="/docs/nfs/v1.7/">NFS</a>
        </div>
      <li class="dropdown">
        <a role="button" href="javascript:void(0)">Community</a>
        <div class="dropdown-content">
          <a href="//github.com/rook/rook">GitHub</a>
          <a href="//slack.rook.io/">Slack</a>
          <a href="//groups.google.com/forum/#!forum/rook-dev">Forum</a>
          <a href="//twitter.com/rook_io">Twitter</a>
        </div>
      </li>
      <li><a href="//blog.rook.io/">Blog</a></li>
      <li><a class="button small" href="/docs/rook/v1.9/quickstart.html">Get Started</a></li>
    </ul>
  </div>
</nav>

    <main id="content" aria-label="Content"><div>



















<section class="docs-header">
  <h1>Ceph</h1>
  <div class="versions">
    <a role="button" href="javascript:void(0)">Rook Ceph v1.5</a>
    <div class="versions-dropdown-content">
      
        <a href="/docs/rook/v1.9/ceph-common-issues.html">Rook Ceph v1.9</a>
      
        <a href="/docs/rook/v1.8/ceph-common-issues.html">Rook Ceph v1.8</a>
      
        <a href="/docs/rook/v1.7/ceph-common-issues.html">Rook Ceph v1.7</a>
      
        <a href="/docs/rook/v1.6/ceph-common-issues.html">Rook Ceph v1.6</a>
      
        <a href="/docs/rook/v1.5/ceph-common-issues.html" class="active">Rook Ceph v1.5</a>
      
        <a href="/docs/rook/v1.4/ceph-common-issues.html">Rook Ceph v1.4</a>
      
        <a href="/docs/rook/v1.3/ceph-common-issues.html">Rook Ceph v1.3</a>
      
        <a href="/docs/rook/v1.2/ceph-common-issues.html">Rook Ceph v1.2</a>
      
        <a href="/docs/rook/v1.1/ceph-common-issues.html">Rook Ceph v1.1</a>
      
        <a href="/docs/rook/v1.0/ceph-common-issues.html">Rook Ceph v1.0</a>
      
        <a href="/docs/rook/v0.9/ceph-common-issues.html">Rook Ceph v0.9</a>
      
        <a href="/docs/rook/v0.8/ceph-common-issues.html">Rook Ceph v0.8</a>
      
        <a href="/docs/rook/v0.7/ceph-common-issues.html">Rook Ceph v0.7</a>
      
        <a href="/docs/rook/v0.6/ceph-common-issues.html">Rook Ceph v0.6</a>
      
        <a href="/docs/rook/v0.5/ceph-common-issues.html">Rook Ceph v0.5</a>
      
        <a href="/docs/rook/latest/ceph-common-issues.html">Rook Ceph latest</a>
      
    </div>
    <img src="/images/arrow.svg" />
  </div>
</section>
<div class="page">
  <div class="docs-menu">
      <ul id="docs-ul"></ul>
  </div>
  <div class="docs-content">
    <div class="docs-actions">
      <a id="edit" href="https://github.com/rook/rook/blob/master/Documentation/ceph-common-issues.md">Edit on GitHub</a>
    </div>
    
      <div class="alert old">
        <p><b>PLEASE NOTE</b>: This document applies to v1.5 version and not to the latest <strong>stable</strong> release v1.9</p>
      </div>
    
    <div class="docs-text">
      <h1 id="ceph-common-issues">Ceph Common Issues</h1>

<p>Many of these problem cases are hard to summarize down to a short phrase that adequately describes the problem. Each problem will start with a bulleted list of symptoms. Keep in mind that all symptoms may not apply depending on the configuration of Rook. If the majority of the symptoms are seen there is a fair chance you are experiencing that problem.</p>

<p>If after trying the suggestions found on this page and the problem is not resolved, the Rook team is very happy to help you troubleshoot the issues in their Slack channel. Once you have <a href="https://slack.rook.io">registered for the Rook Slack</a>, proceed to the <code class="language-plaintext highlighter-rouge">#ceph</code> channel to ask for assistance.</p>

<h2 id="table-of-contents-">Table of Contents <!-- omit in toc --></h2>

<ul>
  <li><a href="#troubleshooting-techniques">Troubleshooting Techniques</a></li>
  <li><a href="#pod-using-ceph-storage-is-not-running">Pod Using Ceph Storage Is Not Running</a></li>
  <li><a href="#cluster-failing-to-service-requests">Cluster failing to service requests</a></li>
  <li><a href="#monitors-are-the-only-pods-running">Monitors are the only pods running</a></li>
  <li><a href="#pvcs-stay-in-pending-state">PVCs stay in pending state</a></li>
  <li><a href="#osd-pods-are-failing-to-start">OSD pods are failing to start</a></li>
  <li><a href="#osd-pods-are-not-created-on-my-devices">OSD pods are not created on my devices</a></li>
  <li><a href="#node-hangs-after-reboot">Node hangs after reboot</a></li>
  <li><a href="#rook-agent-modprobe-exec-format-error">Rook Agent modprobe exec format error</a></li>
  <li><a href="#rook-agent-rbd-module-missing-error">Rook Agent rbd module missing error</a></li>
  <li><a href="#using-multiple-shared-filesystem-cephfs-is-attempted-on-a-kernel-version-older-than-47">Using multiple shared filesystem (CephFS) is attempted on a kernel version older than 4.7</a></li>
  <li><a href="#set-debug-log-level-for-all-ceph-daemons">Set debug log level for all Ceph daemons</a></li>
  <li><a href="#activate-log-to-file-for-a-particular-ceph-daemon">Activate log to file for a particular Ceph daemon</a></li>
  <li><a href="#flex-storage-class-versus-ceph-csi-storage-class">Flex storage class versus Ceph CSI storage class</a></li>
  <li><a href="#a-worker-node-using-rbd-devices-hangs-up">A worker node using RBD devices hangs up</a></li>
  <li><a href="#too-few-pgs-per-osd-warning-is-shown">Too few PGs per OSD warning is shown</a></li>
  <li><a href="#lvm-metadata-can-be-corrupted-with-osd-on-lv-backed-pvc">LVM metadata can be corrupted with OSD on LV-backed PVC</a></li>
  <li><a href="#osd-prepare-job-fails-due-to-low-aio-max-nr-setting">OSD prepare job fails due to low aio-max-nr setting</a></li>
  <li><a href="#failed-to-create-crds">Failed to create CRDs</a></li>
</ul>

<p>See also the <a href="/docs/rook/v1.5/ceph-csi-troubleshooting.html">CSI Troubleshooting Guide</a>.</p>

<h2 id="troubleshooting-techniques">Troubleshooting Techniques</h2>

<p>There are two main categories of information you will need to investigate issues in the cluster:</p>

<ol>
  <li>Kubernetes status and logs documented <a href="/docs/rook/v1.5/common-issues.html">here</a></li>
  <li>Ceph cluster status (see upcoming <a href="#ceph-tools">Ceph tools</a> section)</li>
</ol>

<h3 id="ceph-tools">Ceph Tools</h3>

<p>After you verify the basic health of the running pods, next you will want to run Ceph tools for status of the storage components. There are two ways to run the Ceph tools, either in the Rook toolbox or inside other Rook pods that are already running.</p>

<ul>
  <li>Logs on a specific node to find why a PVC is failing to mount:
    <ul>
      <li>Rook agent errors around the attach/detach: <code class="language-plaintext highlighter-rouge">kubectl logs -n rook-ceph &lt;rook-ceph-agent-pod&gt;</code></li>
    </ul>
  </li>
  <li>See the <a href="/docs/rook/v1.5/ceph-advanced-configuration.html#log-collection">log collection topic</a> for a script that will help you gather the logs</li>
  <li>Other artifacts:
    <ul>
      <li>The monitors that are expected to be in quorum: <code class="language-plaintext highlighter-rouge">kubectl -n &lt;cluster-namespace&gt; get configmap rook-ceph-mon-endpoints -o yaml | grep data</code></li>
    </ul>
  </li>
</ul>

<h4 id="tools-in-the-rook-toolbox">Tools in the Rook Toolbox</h4>

<p>The <a href="/docs/rook/v1.5/ceph-toolbox.html">rook-ceph-tools pod</a> provides a simple environment to run Ceph tools. Once the pod is up and running, connect to the pod to execute Ceph commands to evaluate that current state of the cluster.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">kubectl -n rook-ceph exec -it $</span><span class="o">(</span>kubectl <span class="nt">-n</span> rook-ceph get pod <span class="nt">-l</span> <span class="s2">"app=rook-ceph-tools"</span> <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.items[0].metadata.name}'</span><span class="o">)</span> bash
</code></pre></div></div>

<h4 id="ceph-commands">Ceph Commands</h4>

<p>Here are some common commands to troubleshoot a Ceph cluster:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ceph status</code></li>
  <li><code class="language-plaintext highlighter-rouge">ceph osd status</code></li>
  <li><code class="language-plaintext highlighter-rouge">ceph osd df</code></li>
  <li><code class="language-plaintext highlighter-rouge">ceph osd utilization</code></li>
  <li><code class="language-plaintext highlighter-rouge">ceph osd pool stats</code></li>
  <li><code class="language-plaintext highlighter-rouge">ceph osd tree</code></li>
  <li><code class="language-plaintext highlighter-rouge">ceph pg stat</code></li>
</ul>

<p>The first two status commands provide the overall cluster health. The normal state for cluster operations is HEALTH_OK, but will still function when the state is in a HEALTH_WARN state. If you are in a WARN state, then the cluster is in a condition that it may enter the HEALTH_ERROR state at which point <em>all</em> disk I/O operations are halted. If a HEALTH_WARN state is observed, then one should take action to prevent the cluster from halting when it enters the HEALTH_ERROR state.</p>

<p>There are many Ceph sub-commands to look at and manipulate Ceph objects, well beyond the scope this document. See the <a href="https://docs.ceph.com/">Ceph documentation</a> for more details of gathering information about the health of the cluster. In addition, there are other helpful hints and some best practices located in the <a href="advanced-configuration.md">Advanced Configuration section</a>. Of particular note, there are scripts for collecting logs and gathering OSD information there.</p>

<h2 id="pod-using-ceph-storage-is-not-running">Pod Using Ceph Storage Is Not Running</h2>

<blockquote>
  <p>This topic is specific to creating PVCs based on Rook’s <strong>Flex</strong> driver, which is no longer the default option.
By default, Rook deploys the CSI driver for binding the PVCs to the storage.</p>
</blockquote>

<h3 id="symptoms">Symptoms</h3>

<ul>
  <li>The pod that is configured to use Rook storage is stuck in the <code class="language-plaintext highlighter-rouge">ContainerCreating</code> status</li>
  <li><code class="language-plaintext highlighter-rouge">kubectl describe pod</code> for the pod mentions one or more of the following:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">PersistentVolumeClaim is not bound</code></li>
      <li><code class="language-plaintext highlighter-rouge">timeout expired waiting for volumes to attach/mount</code></li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">kubectl -n rook-ceph get pod</code> shows the rook-ceph-agent pods in a <code class="language-plaintext highlighter-rouge">CrashLoopBackOff</code> status</li>
</ul>

<p>If you see that the PVC remains in <strong>pending</strong> state, see the topic <a href="#pvcs-stay-in-pending-state">PVCs stay in pending state</a>.</p>

<h3 id="possible-solutions-summary">Possible Solutions Summary</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">rook-ceph-agent</code> pod is in a <code class="language-plaintext highlighter-rouge">CrashLoopBackOff</code> status because it cannot deploy its driver on a read-only filesystem: <a href="/docs/rook/v1.5/ceph-prerequisites.html#ceph-flexvolume-configuration">Flexvolume configuration pre-reqs</a></li>
  <li>Persistent Volume and/or Claim are failing to be created and bound: <a href="#volume-creation">Volume Creation</a></li>
  <li><code class="language-plaintext highlighter-rouge">rook-ceph-agent</code> pod is failing to mount and format the volume: <a href="#volume-mounting">Rook Agent Mounting</a></li>
</ul>

<h3 id="investigation-details">Investigation Details</h3>

<p>If you see some of the symptoms above, it’s because the requested Rook storage for your pod is not being created and mounted successfully.
In this walkthrough, we will be looking at the wordpress mysql example pod that is failing to start.</p>

<p>To first confirm there is an issue, you can run commands similar to the following and you should see similar output (note that some of it has been omitted for brevity):</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">&gt;</span><span class="w"> </span>kubectl get pod
<span class="go">NAME                              READY     STATUS              RESTARTS   AGE
wordpress-mysql-918363043-50pjr   0/1       ContainerCreating   0          1h

</span><span class="gp">&gt;</span><span class="w"> </span>kubectl describe pod wordpress-mysql-918363043-50pjr
<span class="c">...
</span><span class="go">Events:
  FirstSeen	LastSeen	Count	From			SubObjectPath	Type		Reason			Message
  ---------	--------	-----	----			-------------	--------	------			-------
  1h		1h		3	default-scheduler			Warning		FailedScheduling	PersistentVolumeClaim is not bound: "mysql-pv-claim" (repeated 2 times)
  1h		35s		36	kubelet, 172.17.8.101			Warning		FailedMount		Unable to mount volumes for pod "wordpress-mysql-918363043-50pjr_default(08d14e75-bd99-11e7-bc4c-001c428b9fc8)": timeout expired waiting for volumes to attach/mount for pod "default"/"wordpress-mysql-918363043-50pjr". list of unattached/unmounted volumes=[mysql-persistent-storage]
  1h		35s		36	kubelet, 172.17.8.101			Warning		FailedSync		Error syncing pod
</span></code></pre></div></div>

<p>To troubleshoot this, let’s walk through the volume provisioning steps in order to confirm where the failure is happening.</p>

<h4 id="ceph-agent-deployment">Ceph Agent Deployment</h4>

<p>The <code class="language-plaintext highlighter-rouge">rook-ceph-agent</code> pods are responsible for mapping and mounting the volume from the cluster onto the node that your pod will be running on.
If the <code class="language-plaintext highlighter-rouge">rook-ceph-agent</code> pod is not running then it cannot perform this function.</p>

<p>Below is an example of the <code class="language-plaintext highlighter-rouge">rook-ceph-agent</code> pods failing to get to the <code class="language-plaintext highlighter-rouge">Running</code> status because they are in a <code class="language-plaintext highlighter-rouge">CrashLoopBackOff</code> status:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">&gt;</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook-ceph get pod
<span class="go">NAME                                  READY     STATUS             RESTARTS   AGE
rook-ceph-agent-ct5pj                 0/1       CrashLoopBackOff   16         59m
rook-ceph-agent-zb6n9                 0/1       CrashLoopBackOff   16         59m
rook-operator-2203999069-pmhzn        1/1       Running            0          59m
</span></code></pre></div></div>

<p>If you see this occurring, you can get more details about why the <code class="language-plaintext highlighter-rouge">rook-ceph-agent</code> pods are continuing to crash with the following command and its sample output:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">&gt;</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook-ceph get pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>rook-ceph-agent <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{range .items[*]}{.metadata.name}{"\t"}{.status.containerStatuses[0].lastState.terminated.message}{"\n"}{end}'</span>
<span class="go">rook-ceph-agent-ct5pj	mkdir /usr/libexec/kubernetes: read-only filesystem
rook-ceph-agent-zb6n9	mkdir /usr/libexec/kubernetes: read-only filesystem
</span></code></pre></div></div>

<p>From the output above, we can see that the agents were not able to bind mount to <code class="language-plaintext highlighter-rouge">/usr/libexec/kubernetes</code> on the host they are scheduled to run on.
For some environments, this default path is read-only and therefore a better path must be provided to the agents.</p>

<p>First, clean up the agent deployment with:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl -n rook-ceph delete daemonset rook-ceph-agent
</span></code></pre></div></div>

<p>Once the <code class="language-plaintext highlighter-rouge">rook-ceph-agent</code> pods are gone, <strong>follow the instructions in the <a href="/docs/rook/v1.5/ceph-prerequisites.html#ceph-flexvolume-configuration">Flexvolume configuration pre-reqs</a></strong> to ensure a good value for <code class="language-plaintext highlighter-rouge">--volume-plugin-dir</code> has been provided to the Kubelet.
After that has been configured, and the Kubelet has been restarted, start the agent pods up again by restarting <code class="language-plaintext highlighter-rouge">rook-operator</code>:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl -n rook-ceph delete pod -l app=rook-ceph-operator
</span></code></pre></div></div>

<h4 id="volume-creation">Volume Creation</h4>

<p>The volume must first be created in the Rook cluster and then bound to a volume claim before it can be mounted to a pod.
Let’s confirm that with the following commands and their output:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">&gt;</span><span class="w"> </span>kubectl get pv
<span class="go">NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS     CLAIM                    STORAGECLASS   REASON    AGE
pvc-9f273fbc-bdbf-11e7-bc4c-001c428b9fc8   20Gi       RWO           Delete          Bound      default/mysql-pv-claim   rook-ceph-block               25m

</span><span class="gp">&gt;</span><span class="w"> </span>kubectl get pvc
<span class="go">NAME             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS   AGE
mysql-pv-claim   Bound     pvc-9f273fbc-bdbf-11e7-bc4c-001c428b9fc8   20Gi       RWO           rook-ceph-block     25m
</span></code></pre></div></div>

<p>Both your volume and its claim should be in the <code class="language-plaintext highlighter-rouge">Bound</code> status.
If one or neither of them is not in the <code class="language-plaintext highlighter-rouge">Bound</code> status, then look for details of the issue in the <code class="language-plaintext highlighter-rouge">rook-operator</code> logs:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl -n rook-ceph logs `kubectl -n rook-ceph -l app=rook-ceph-operator get pods -o jsonpath='{.items[*].metadata.name}'`
</span></code></pre></div></div>

<p>If the volume is failing to be created, there should be details in the <code class="language-plaintext highlighter-rouge">rook-operator</code> log output, especially those tagged with <code class="language-plaintext highlighter-rouge">op-provisioner</code>.</p>

<p>One common cause for the <code class="language-plaintext highlighter-rouge">rook-operator</code> failing to create the volume is when the <code class="language-plaintext highlighter-rouge">clusterNamespace</code> field of the <code class="language-plaintext highlighter-rouge">StorageClass</code> doesn’t match the <strong>namespace</strong> of the Rook cluster, as described in <a href="https://github.com/rook/rook/issues/1502">#1502</a>.
In that scenario, the <code class="language-plaintext highlighter-rouge">rook-operator</code> log would show a failure similar to the following:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">2018-03-28 18:58:32.041603 I | op-provisioner: creating volume with configuration {pool:replicapool clusterNamespace:rook-ceph fstype:}
2018-03-28 18:58:32.041728 I | exec: Running command: rbd create replicapool/pvc-fd8aba49-32b9-11e8-978e-08002762c796 --size 20480 --cluster=rook --conf=/var/lib/rook/rook-ceph/rook.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
E0328 18:58:32.060893       5 controller.go:801] Failed to provision volume for claim "default/mysql-pv-claim" with StorageClass "rook-ceph-block": Failed to create rook block image replicapool/pvc-fd8aba49-32b9-11e8-978e-08002762c796: failed to create image pvc-fd8aba49-32b9-11e8-978e-08002762c796 in pool replicapool of size 21474836480: Failed to complete '': exit status 1. global_init: unable to open config file from search list /var/lib/rook/rook-ceph/rook.config
. output:
</span></code></pre></div></div>

<p>The solution is to ensure that the <a href="https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/flex/storageclass.yaml#L28"><code class="language-plaintext highlighter-rouge">clusterNamespace</code></a> field matches the <strong>namespace</strong> of the Rook cluster when creating the <code class="language-plaintext highlighter-rouge">StorageClass</code>.</p>

<h4 id="volume-mounting">Volume Mounting</h4>

<p>The final step in preparing Rook storage for your pod is for the <code class="language-plaintext highlighter-rouge">rook-ceph-agent</code> pod to mount and format it.
If all the preceding sections have been successful or inconclusive, then take a look at the <code class="language-plaintext highlighter-rouge">rook-ceph-agent</code> pod logs for further clues.
You can determine which <code class="language-plaintext highlighter-rouge">rook-ceph-agent</code> is running on the same node that your pod is scheduled on by using the <code class="language-plaintext highlighter-rouge">-o wide</code> output, then you can get the logs for that <code class="language-plaintext highlighter-rouge">rook-ceph-agent</code> pod similar to the example below:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">&gt;</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook-ceph get pod <span class="nt">-o</span> wide
<span class="go">NAME                                  READY     STATUS    RESTARTS   AGE       IP             NODE
rook-ceph-agent-h6scx                 1/1       Running   0          9m        172.17.8.102   172.17.8.102
rook-ceph-agent-mp7tn                 1/1       Running   0          9m        172.17.8.101   172.17.8.101
rook-operator-2203999069-3tb68        1/1       Running   0          9m        10.32.0.7      172.17.8.101

</span><span class="gp">&gt;</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook-ceph logs rook-ceph-agent-h6scx
<span class="go">2017-10-30 23:07:06.984108 I | rook: starting Rook v0.5.0-241.g48ce6de.dirty with arguments '/usr/local/bin/rook agent'
[...]
</span></code></pre></div></div>

<p>In the <code class="language-plaintext highlighter-rouge">rook-ceph-agent</code> pod logs, you may see a snippet similar to the following:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">Failed to complete rbd: signal: interrupt.
</span></code></pre></div></div>

<p>In this case, the agent waited for the <code class="language-plaintext highlighter-rouge">rbd</code> command but it did not finish in a timely manner so the agent gave up and stopped it.
This can happen for multiple reasons, but using <code class="language-plaintext highlighter-rouge">dmesg</code> will likely give you insight into the root cause.
If <code class="language-plaintext highlighter-rouge">dmesg</code> shows something similar to below, then it means you have an old kernel that can’t talk to the cluster:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">libceph: mon2 10.205.92.13:6789 feature set mismatch, my 4a042a42 &lt; server's 2004a042a42, missing 20000000000
</span></code></pre></div></div>

<p>If <code class="language-plaintext highlighter-rouge">uname -a</code> shows that you have a kernel version older than <code class="language-plaintext highlighter-rouge">3.15</code>, you’ll need to perform <strong>one</strong> of the following:</p>

<ul>
  <li>Disable some Ceph features by starting the <a href="/docs/rook/v1.5/ceph-toolbox.html">rook toolbox</a> and running <code class="language-plaintext highlighter-rouge">ceph osd crush tunables bobtail</code></li>
  <li>Upgrade your kernel to <code class="language-plaintext highlighter-rouge">3.15</code> or later.</li>
</ul>

<h4 id="filesystem-mounting">Filesystem Mounting</h4>

<p>In the <code class="language-plaintext highlighter-rouge">rook-ceph-agent</code> pod logs, you may see a snippet similar to the following:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">2017-11-07 00:04:37.808870 I | rook-flexdriver: WARNING: The node kernel version is 4.4.0-87-generic, which do not support multiple ceph filesystems. The kernel version has to be at least 4.7. If you have multiple ceph filesystems, the result could be inconsistent
</span></code></pre></div></div>

<p>This will happen in kernels with versions older than 4.7, where the option <code class="language-plaintext highlighter-rouge">mds_namespace</code> is not supported. This option is used to specify a filesystem namespace.</p>

<p>In this case, if there is only one filesystem in the Rook cluster, there should be no issues and the mount should succeed. If you have more than one filesystem, inconsistent results may arise and the filesystem mounted may not be the one you specified.</p>

<p>If the issue is still not resolved from the steps above, please come chat with us on the <strong>#general</strong> channel of our <a href="https://slack.rook.io">Rook Slack</a>.
We want to help you get your storage working and learn from those lessons to prevent users in the future from seeing the same issue.</p>

<h2 id="cluster-failing-to-service-requests">Cluster failing to service requests</h2>

<h3 id="symptoms-1">Symptoms</h3>

<ul>
  <li>Execution of the <code class="language-plaintext highlighter-rouge">ceph</code> command hangs</li>
  <li>PersistentVolumes are not being created</li>
  <li>Large amount of slow requests are blocking</li>
  <li>Large amount of stuck requests are blocking</li>
  <li>One or more MONs are restarting periodically</li>
</ul>

<h3 id="investigation">Investigation</h3>

<p>Create a <a href="/docs/rook/v1.5/ceph-toolbox.html">rook-ceph-tools pod</a> to investigate the current state of Ceph. Here is an example of what one might see. In this case the <code class="language-plaintext highlighter-rouge">ceph status</code> command would just hang so a CTRL-C needed to be sent.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook-ceph <span class="nb">exec</span> <span class="nt">-it</span> <span class="si">$(</span>kubectl <span class="nt">-n</span> rook-ceph get pod <span class="nt">-l</span> <span class="s2">"app=rook-ceph-tools"</span> <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.items[0].metadata.name}'</span><span class="si">)</span> bash
<span class="gp">root@rook-ceph-tools:/#</span><span class="w"> </span>ceph status
<span class="go">^CCluster connection interrupted or timed out
</span></code></pre></div></div>

<p>Another indication is when one or more of the MON pods restart frequently. Note the ‘mon107’ that has only been up for 16 minutes in the following output.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook-ceph get all <span class="nt">-o</span> wide <span class="nt">--show-all</span>
<span class="go">NAME                                 READY     STATUS    RESTARTS   AGE       IP               NODE
po/rook-ceph-mgr0-2487684371-gzlbq   1/1       Running   0          17h       192.168.224.46   k8-host-0402
po/rook-ceph-mon107-p74rj            1/1       Running   0          16m       192.168.224.28   k8-host-0402
rook-ceph-mon1-56fgm                 1/1       Running   0          2d        192.168.91.135   k8-host-0404
rook-ceph-mon2-rlxcd                 1/1       Running   0          2d        192.168.123.33   k8-host-0403
rook-ceph-osd-bg2vj                  1/1       Running   0          2d        192.168.91.177   k8-host-0404
rook-ceph-osd-mwxdm                  1/1       Running   0          2d        192.168.123.31   k8-host-0403
</span></code></pre></div></div>

<h3 id="solution">Solution</h3>

<p>What is happening here is that the MON pods are restarting and one or more of the Ceph daemons are not getting configured with the proper cluster information. This is commonly the result of not specifying a value for <code class="language-plaintext highlighter-rouge">dataDirHostPath</code> in your Cluster CRD.</p>

<p>The <code class="language-plaintext highlighter-rouge">dataDirHostPath</code> setting specifies a path on the local host for the Ceph daemons to store configuration and data. Setting this to a path like <code class="language-plaintext highlighter-rouge">/var/lib/rook</code>, reapplying your Cluster CRD and restarting all the Ceph daemons (MON, MGR, OSD, RGW) should solve this problem. After the Ceph daemons have been restarted, it is advisable to restart the <a href="./toolbox.md">rook-tool pod</a>.</p>

<h2 id="monitors-are-the-only-pods-running">Monitors are the only pods running</h2>

<h3 id="symptoms-2">Symptoms</h3>

<ul>
  <li>Rook operator is running</li>
  <li>Either a single mon starts or the mons start very slowly (at least several minutes apart)</li>
  <li>The crash-collector pods are crashing</li>
  <li>No mgr, osd, or other daemons are created except the CSI driver</li>
</ul>

<h3 id="investigation-1">Investigation</h3>

<p>When the operator is starting a cluster, the operator will start one mon at a time and check that they are healthy before continuing to bring up all three mons.
If the first mon is not detected healthy, the operator will continue to check until it is healthy. If the first mon fails to start, a second and then a third
mon may attempt to start. However, they will never form quorum and the orchestration will be blocked from proceeding.</p>

<p>The crash-collector pods will be blocked from starting until the mons have formed quorum the first time.</p>

<p>There are several common causes for the mons failing to form quorum:</p>

<ul>
  <li>The operator pod does not have network connectivity to the mon pod(s). The network may be configured incorrectly.</li>
  <li>One or more mon pods are in running state, but the operator log shows they are not able to form quorum</li>
  <li>A mon is using configuration from a previous installation. See the <a href="/docs/rook/v1.5/ceph-teardown.html#delete-the-data-on-hosts">cleanup guide</a>
for cleaning the previous cluster.</li>
  <li>A firewall may be blocking the ports required for the Ceph mons to form quorum. Ensure ports 6789 and 3300 are enabled.
See the <a href="https://docs.ceph.com/en/latest/rados/configuration/network-config-ref/">Ceph networking guide</a> for more details.</li>
</ul>

<h4 id="operator-fails-to-connect-to-the-mon">Operator fails to connect to the mon</h4>

<p>First look at the logs of the operator to confirm if it is able to connect to the mons.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl -n rook-ceph logs -l app=rook-ceph-operator
</span></code></pre></div></div>

<p>Likely you will see an error similar to the following that the operator is timing out when connecting to the mon. The last command is <code class="language-plaintext highlighter-rouge">ceph mon_status</code>,
followed by a timeout message five minutes later.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">2018-01-21 21:47:32.375833 I | exec: Running command: ceph mon_status --cluster=rook --conf=/var/lib/rook/rook-ceph/rook.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/442263890
2018-01-21 21:52:35.370533 I | exec: 2018-01-21 21:52:35.071462 7f96a3b82700  0 monclient(hunting): authenticate timed out after 300
2018-01-21 21:52:35.071462 7f96a3b82700  0 monclient(hunting): authenticate timed out after 300
2018-01-21 21:52:35.071524 7f96a3b82700  0 librados: client.admin authentication error (110) Connection timed out
2018-01-21 21:52:35.071524 7f96a3b82700  0 librados: client.admin authentication error (110) Connection timed out
[errno 110] error connecting to the cluster
</span></code></pre></div></div>

<p>The error would appear to be an authentication error, but it is misleading. The real issue is a timeout.</p>

<h4 id="solution-1">Solution</h4>

<p>If you see the timeout in the operator log, verify if the mon pod is running (see the next section).
If the mon pod is running, check the network connectivity between the operator pod and the mon pod.
A common issue is that the CNI is not configured correctly.</p>

<p>To verify the network connectivity:</p>
<ul>
  <li>Get the endpoint for a mon</li>
  <li>Curl the mon from the operator pod</li>
</ul>

<p>For example, this command will curl the first mon from the operator:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl -n rook-ceph exec deploy/rook-ceph-operator -- curl $(kubectl -n rook-ceph get svc -l app=rook-ceph-mon -o jsonpath='{.items[0].spec.clusterIP}'):3300 2&gt;/dev/null
</code></pre></div></div>

<blockquote>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ceph v2
</code></pre></div>  </div>
</blockquote>

<p>If “ceph v2” is printed to the console, the connection was successful. If the command does not respond or
otherwise fails, the network connection cannot be established.</p>

<h4 id="failing-mon-pod">Failing mon pod</h4>

<p>Second we need to verify if the mon pod started successfully.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook-ceph get pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>rook-ceph-mon
<span class="go">NAME                                READY     STATUS               RESTARTS   AGE
rook-ceph-mon-a-69fb9c78cd-58szd    1/1       CrashLoopBackOff     2          47s
</span></code></pre></div></div>

<p>If the mon pod is failing as in this example, you will need to look at the mon pod status or logs to determine the cause. If the pod is in a crash loop backoff state,
you should see the reason by describing the pod.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>The pod shows a termination status that the keyring does not match the existing keyring
<span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook-ceph describe pod <span class="nt">-l</span> <span class="nv">mon</span><span class="o">=</span>rook-ceph-mon0
<span class="c">...
</span><span class="go">    Last State:    Terminated
      Reason:    Error
      Message:    The keyring does not match the existing keyring in /var/lib/rook/rook-ceph-mon0/data/keyring.
                    You may need to delete the contents of dataDirHostPath on the host from a previous deployment.
</span><span class="c">...
</span></code></pre></div></div>

<p>See the solution in the next section regarding cleaning up the <code class="language-plaintext highlighter-rouge">dataDirHostPath</code> on the nodes.</p>

<h4 id="solution-2">Solution</h4>

<p>This is a common problem reinitializing the Rook cluster when the local directory used for persistence has <strong>not</strong> been purged.
This directory is the <code class="language-plaintext highlighter-rouge">dataDirHostPath</code> setting in the cluster CRD and is typically set to <code class="language-plaintext highlighter-rouge">/var/lib/rook</code>.
To fix the issue you will need to delete all components of Rook and then delete the contents of <code class="language-plaintext highlighter-rouge">/var/lib/rook</code> (or the directory specified by <code class="language-plaintext highlighter-rouge">dataDirHostPath</code>) on each of the hosts in the cluster.
Then when the cluster CRD is applied to start a new cluster, the rook-operator should start all the pods as expected.</p>

<blockquote>
  <p><strong>IMPORTANT: Deleting the <code class="language-plaintext highlighter-rouge">dataDirHostPath</code> folder is destructive to the storage. Only delete the folder if you are trying to permanently purge the Rook cluster.</strong></p>
</blockquote>

<p>See the <a href="/docs/rook/v1.5/ceph-teardown.html">Cleanup Guide</a> for more details.</p>

<h2 id="pvcs-stay-in-pending-state">PVCs stay in pending state</h2>

<h3 id="symptoms-3">Symptoms</h3>

<ul>
  <li>When you create a PVC based on a rook storage class, it stays pending indefinitely</li>
</ul>

<p>For the Wordpress example, you might see two PVCs in pending state.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>kubectl get pvc
<span class="go">NAME             STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS      AGE
mysql-pv-claim   Pending                                      rook-ceph-block   8s
wp-pv-claim      Pending                                      rook-ceph-block   16s
</span></code></pre></div></div>

<h3 id="investigation-2">Investigation</h3>

<p>There are two common causes for the PVCs staying in pending state:</p>

<ol>
  <li>There are no OSDs in the cluster</li>
  <li>The CSI provisioner pod is not running or is not responding to the request to provision the storage</li>
</ol>

<p>If you are still using the Rook flex driver for the volumes (the CSI driver is the default since Rook v1.1),
another cause could be that the operator is not running or is otherwise not responding to the request to provision the storage.</p>

<h4 id="confirm-if-there-are-osds">Confirm if there are OSDs</h4>

<p>To confirm if you have OSDs in your cluster, connect to the <a href="/docs/rook/v1.5/ceph-toolbox.html">Rook Toolbox</a> and run the <code class="language-plaintext highlighter-rouge">ceph status</code> command.
You should see that you have at least one OSD <code class="language-plaintext highlighter-rouge">up</code> and <code class="language-plaintext highlighter-rouge">in</code>. The minimum number of OSDs required depends on the
<code class="language-plaintext highlighter-rouge">replicated.size</code> setting in the pool created for the storage class. In a “test” cluster, only one OSD is required
(see <code class="language-plaintext highlighter-rouge">storageclass-test.yaml</code>). In the production storage class example (<code class="language-plaintext highlighter-rouge">storageclass.yaml</code>), three OSDs would be required.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>ceph status
<span class="go">  cluster:
    id:     a0452c76-30d9-4c1a-a948-5d8405f19a7c
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum a,b,c (age 11m)
    mgr: a(active, since 10m)
    osd: 1 osds: 1 up (since 46s), 1 in (since 109m)
</span></code></pre></div></div>

<h4 id="osd-prepare-logs">OSD Prepare Logs</h4>

<p>If you don’t see the expected number of OSDs, let’s investigate why they weren’t created.
On each node where Rook looks for OSDs to configure, you will see an “osd prepare” pod.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook-ceph get pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>rook-ceph-osd-prepare
<span class="go">NAME                                 ...  READY   STATUS      RESTARTS   AGE
rook-ceph-osd-prepare-minikube-9twvk   0/2     Completed   0          30m
</span></code></pre></div></div>

<p>See the section on <a href="#osd-pods-are-not-created-on-my-devices">why OSDs are not getting created</a> to investigate the logs.</p>

<h4 id="csi-driver">CSI Driver</h4>

<p>The CSI driver may not be responding to the requests. Look in the logs of the CSI provisioner pod to see if there are any errors
during the provisioning.</p>

<p>There are two provisioner pods:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl -n rook-ceph get pod -l app=csi-rbdplugin-provisioner
</code></pre></div></div>

<p>Get the logs of each of the pods. One of them should be the “leader” and be responding to requests.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl -n rook-ceph logs csi-cephfsplugin-provisioner-d77bb49c6-q9hwq csi-provisioner
</code></pre></div></div>

<p>See also the <a href="/docs/rook/v1.5/ceph-csi-troubleshooting.html">CSI Troubleshooting Guide</a>.</p>

<h4 id="operator-unresponsiveness">Operator unresponsiveness</h4>

<p>Lastly, if you have OSDs <code class="language-plaintext highlighter-rouge">up</code> and <code class="language-plaintext highlighter-rouge">in</code>, the next step is to confirm the operator is responding to the requests.
Look in the Operator pod logs around the time when the PVC was created to confirm if the request is being raised.
If the operator does not show requests to provision the block image, the operator may be stuck on some other operation.
In this case, restart the operator pod to get things going again.</p>

<h3 id="solution-3">Solution</h3>

<p>If the “osd prepare” logs didn’t give you enough clues about why the OSDs were not being created,
please review your <a href="eph-cluster-crd.html#storage-selection-settings">cluster.yaml</a> configuration.
The common misconfigurations include:</p>

<ul>
  <li>If <code class="language-plaintext highlighter-rouge">useAllDevices: true</code>, Rook expects to find local devices attached to the nodes. If no devices are found, no OSDs will be created.</li>
  <li>If <code class="language-plaintext highlighter-rouge">useAllDevices: false</code>, OSDs will only be created if <code class="language-plaintext highlighter-rouge">deviceFilter</code> is specified.</li>
  <li>Only local devices attached to the nodes will be configurable by Rook. In other words, the devices must show up under <code class="language-plaintext highlighter-rouge">/dev</code>.
    <ul>
      <li>The devices must not have any partitions or filesystems on them. Rook will only configure raw devices. Partitions are not yet supported.</li>
    </ul>
  </li>
</ul>

<h2 id="osd-pods-are-failing-to-start">OSD pods are failing to start</h2>

<h3 id="symptoms-4">Symptoms</h3>

<ul>
  <li>OSD pods are failing to start</li>
  <li>You have started a cluster after tearing down another cluster</li>
</ul>

<h3 id="investigation-3">Investigation</h3>

<p>When an OSD starts, the device or directory will be configured for consumption. If there is an error with the configuration, the pod will crash and you will see the CrashLoopBackoff
status for the pod. Look in the osd pod logs for an indication of the failure.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook-ceph logs rook-ceph-osd-fl8fs
<span class="c">...
</span></code></pre></div></div>

<p>One common case for failure is that you have re-deployed a test cluster and some state may remain from a previous deployment.
If your cluster is larger than a few nodes, you may get lucky enough that the monitors were able to start and form quorum. However, now the OSDs pods may fail to start due to the
old state. Looking at the OSD pod logs you will see an error about the file already existing.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook-ceph logs rook-ceph-osd-fl8fs
<span class="c">...
</span><span class="go">2017-10-31 20:13:11.187106 I | mkfs-osd0: 2017-10-31 20:13:11.186992 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) _read_fsid unparsable uuid
2017-10-31 20:13:11.187208 I | mkfs-osd0: 2017-10-31 20:13:11.187026 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) _setup_block_symlink_or_file failed to create block symlink to /dev/disk/by-partuuid/651153ba-2dfc-4231-ba06-94759e5ba273: (17) File exists
2017-10-31 20:13:11.187233 I | mkfs-osd0: 2017-10-31 20:13:11.187038 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) mkfs failed, (17) File exists
2017-10-31 20:13:11.187254 I | mkfs-osd0: 2017-10-31 20:13:11.187042 7f0059d62e00 -1 OSD::mkfs: ObjectStore::mkfs failed with error (17) File exists
2017-10-31 20:13:11.187275 I | mkfs-osd0: 2017-10-31 20:13:11.187121 7f0059d62e00 -1  ** ERROR: error creating empty object store in /var/lib/rook/osd0: (17) File exists
</span></code></pre></div></div>

<h3 id="solution-4">Solution</h3>

<p>If the error is from the file that already exists, this is a common problem reinitializing the Rook cluster when the local directory used for persistence has <strong>not</strong> been purged.
This directory is the <code class="language-plaintext highlighter-rouge">dataDirHostPath</code> setting in the cluster CRD and is typically set to <code class="language-plaintext highlighter-rouge">/var/lib/rook</code>.
To fix the issue you will need to delete all components of Rook and then delete the contents of <code class="language-plaintext highlighter-rouge">/var/lib/rook</code> (or the directory specified by <code class="language-plaintext highlighter-rouge">dataDirHostPath</code>) on each of the hosts in the cluster.
Then when the cluster CRD is applied to start a new cluster, the rook-operator should start all the pods as expected.</p>

<h2 id="osd-pods-are-not-created-on-my-devices">OSD pods are not created on my devices</h2>

<h3 id="symptoms-5">Symptoms</h3>

<ul>
  <li>No OSD pods are started in the cluster</li>
  <li>Devices are not configured with OSDs even though specified in the Cluster CRD</li>
  <li>One OSD pod is started on each node instead of multiple pods for each device</li>
</ul>

<h3 id="investigation-4">Investigation</h3>

<p>First, ensure that you have specified the devices correctly in the CRD.
The <a href="/docs/rook/v1.5/ceph-cluster-crd.html#storage-selection-settings">Cluster CRD</a> has several ways to specify the devices that are to be consumed by the Rook storage:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">useAllDevices: true</code>: Rook will consume all devices it determines to be available</li>
  <li><code class="language-plaintext highlighter-rouge">deviceFilter</code>: Consume all devices that match this regular expression</li>
  <li><code class="language-plaintext highlighter-rouge">devices</code>: Explicit list of device names on each node to consume</li>
</ul>

<p>Second, if Rook determines that a device is not available (has existing partitions or a formatted filesystem), Rook will skip consuming the devices.
If Rook is not starting OSDs on the devices you expect, Rook may have skipped it for this reason. To see if a device was skipped, view the OSD preparation log
on the node where the device was skipped. Note that it is completely normal and expected for OSD prepare pod to be in the <code class="language-plaintext highlighter-rouge">completed</code> state.
After the job is complete, Rook leaves the pod around in case the logs need to be investigated.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>Get the prepare pods <span class="k">in </span>the cluster
<span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook-ceph get pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>rook-ceph-osd-prepare
<span class="go">NAME                                   READY     STATUS      RESTARTS   AGE
rook-ceph-osd-prepare-node1-fvmrp      0/1       Completed   0          18m
rook-ceph-osd-prepare-node2-w9xv9      0/1       Completed   0          22m
rook-ceph-osd-prepare-node3-7rgnv      0/1       Completed   0          22m

</span><span class="gp">#</span><span class="w"> </span>view the logs <span class="k">for </span>the node of interest <span class="k">in </span>the <span class="s2">"provision"</span> container
<span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook-ceph logs rook-ceph-osd-prepare-node1-fvmrp provision
<span class="go">[...]
</span></code></pre></div></div>

<p>Here are some key lines to look for in the log:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>A device will be skipped <span class="k">if </span>Rook sees it has partitions or a filesystem
<span class="go">2019-05-30 19:02:57.353171 W | cephosd: skipping device sda that is in use
2019-05-30 19:02:57.452168 W | skipping device "sdb5": ["Used by ceph-disk"]

</span><span class="gp">#</span><span class="w"> </span>Other messages about a disk being unusable by ceph include:
<span class="go">Insufficient space (&lt;5GB) on vgs
Insufficient space (&lt;5GB)
LVM detected
Has BlueStore device label
locked
read-only

</span><span class="gp">#</span><span class="w"> </span>A device is going to be configured
<span class="go">2019-05-30 19:02:57.535598 I | cephosd: device sdc to be configured by ceph-volume

</span><span class="gp">#</span><span class="w"> </span>For each device configured you will see a report printed to the log
<span class="go">2019-05-30 19:02:59.844642 I |   Type            Path                                                    LV Size         % of device
2019-05-30 19:02:59.844651 I | ----------------------------------------------------------------------------------------------------
2019-05-30 19:02:59.844677 I |   [data]          /dev/sdc                                                7.00 GB         100%
</span></code></pre></div></div>

<h3 id="solution-5">Solution</h3>

<p>Either update the CR with the correct settings, or clean the partitions or filesystem from your devices.
To clean devices from a previous install see the <a href="/docs/rook/v1.5/ceph-teardown.html#zapping-devices">cleanup guide</a>.</p>

<p>After the settings are updated or the devices are cleaned, trigger the operator to analyze the devices again by restarting the operator.
Each time the operator starts, it will ensure all the desired devices are configured. The operator does automatically
deploy OSDs in most scenarios, but an operator restart will cover any scenarios that the operator doesn’t detect automatically.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>Restart the operator to ensure devices are configured. A new pod will automatically be started when the current operator pod is deleted.
<span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook-ceph delete pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>rook-ceph-operator
<span class="go">[...]
</span></code></pre></div></div>

<h2 id="node-hangs-after-reboot">Node hangs after reboot</h2>

<p>This issue is fixed in Rook v1.3 or later.</p>

<h3 id="symptoms-6">Symptoms</h3>

<ul>
  <li>After issuing a <code class="language-plaintext highlighter-rouge">reboot</code> command, node never returned online</li>
  <li>Only a power cycle helps</li>
</ul>

<h3 id="investigation-5">Investigation</h3>

<p>On a node running a pod with a Ceph persistent volume</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>mount | <span class="nb">grep </span>rbd
<span class="go">
</span><span class="gp">#</span><span class="w"> </span>_netdev mount option is absent, also occurs <span class="k">for </span>cephfs
<span class="gp">#</span><span class="w"> </span>OS is not aware PV is mounted over network
<span class="go">/dev/rbdx on ... (rw,relatime, ..., noquota)
</span></code></pre></div></div>

<p>When the reboot command is issued, network interfaces are terminated before disks
are unmounted. This results in the node hanging as repeated attempts to unmount
Ceph persistent volumes fail with the following error:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>libceph: connect [monitor-ip]:6789 error -101
</code></pre></div></div>

<h3 id="solution-6">Solution</h3>

<p>The node needs to be <a href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/">drained</a> before reboot. After the successful drain, the node can be rebooted as usual.</p>

<p>Because <code class="language-plaintext highlighter-rouge">kubectl drain</code> command automatically marks the node as unschedulable (<code class="language-plaintext highlighter-rouge">kubectl cordon</code> effect), the node needs to be uncordoned once it’s back online.</p>

<p>Drain the node:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">kubectl drain &lt;node-name&gt;</span><span class="w"> </span><span class="nt">--ignore-daemonsets</span> <span class="nt">--delete-local-data</span>
</code></pre></div></div>

<p>Uncordon the node:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">kubectl uncordon &lt;node-name&gt;</span><span class="w">
</span></code></pre></div></div>

<h2 id="rook-agent-modprobe-exec-format-error">Rook Agent modprobe exec format error</h2>

<h3 id="symptoms-7">Symptoms</h3>

<ul>
  <li>PersistentVolumes from Ceph fail/timeout to mount</li>
  <li>Rook Agent logs contain <code class="language-plaintext highlighter-rouge">modinfo: ERROR: could not get modinfo from 'rbd': Exec format error</code> lines</li>
</ul>

<h3 id="solution-7">Solution</h3>

<p>If it is feasible to upgrade your kernel, you should upgrade to <code class="language-plaintext highlighter-rouge">4.x</code>, even better is &gt;= <code class="language-plaintext highlighter-rouge">4.7</code> due to a feature for CephFS added to the kernel.</p>

<p>If you are unable to upgrade the kernel, you need to go to each host that will consume storage and run:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">modprobe rbd
</span></code></pre></div></div>

<p>This command inserts the <code class="language-plaintext highlighter-rouge">rbd</code> module into the kernel.</p>

<p>To persist this fix, you need to add the <code class="language-plaintext highlighter-rouge">rbd</code> kernel module to either <code class="language-plaintext highlighter-rouge">/etc/modprobe.d/</code> or <code class="language-plaintext highlighter-rouge">/etc/modules-load.d/</code>.
For both paths create a file called <code class="language-plaintext highlighter-rouge">rbd.conf</code> with the following content:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">rbd
</span></code></pre></div></div>

<p>Now when a host is restarted, the module should be loaded automatically.</p>

<h2 id="rook-agent-rbd-module-missing-error">Rook Agent rbd module missing error</h2>

<h3 id="symptoms-8">Symptoms</h3>

<ul>
  <li>Rook Agent in <code class="language-plaintext highlighter-rouge">Error</code> or <code class="language-plaintext highlighter-rouge">CrashLoopBackOff</code> status when deploying the Rook operator with <code class="language-plaintext highlighter-rouge">kubectl create -f operator.yaml</code>:</li>
</ul>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span>kubectl <span class="nt">-n</span> rook-ceph get pod
<span class="go">NAME                                 READY     STATUS    RESTARTS   AGE
rook-ceph-agent-gfrm5                0/1       Error     0          14s
rook-ceph-operator-5f4866946-vmtff   1/1       Running   0          23s
rook-discover-qhx6c                  1/1       Running   0          14s
</span></code></pre></div></div>

<ul>
  <li>Rook Agent logs contain below messages:</li>
</ul>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">2018-08-10 09:09:09.461798 I | exec: Running command: cat /lib/modules/4.15.2/modules.builtin
2018-08-10 09:09:09.473858 I | exec: Running command: modinfo -F parm rbd
2018-08-10 09:09:09.477215 N | ceph-volumeattacher: failed rbd single_major check, assuming it's unsupported: failed to check for rbd module single_major param: Failed to complete 'check kmod param': exit status 1. modinfo: ERROR: Module rbd not found.
2018-08-10 09:09:09.477239 I | exec: Running command: modprobe rbd
2018-08-10 09:09:09.480353 I | modprobe rbd: modprobe: FATAL: Module rbd not found.
2018-08-10 09:09:09.480452 N | ceph-volumeattacher: failed to load kernel module rbd: failed to load kernel module rbd: Failed to complete 'modprobe rbd': exit status 1.
failed to run rook ceph agent. failed to create volume manager: failed to load kernel module rbd: Failed to complete 'modprobe rbd': exit status 1.
</span></code></pre></div></div>

<h3 id="solution-8">Solution</h3>

<p>From the log message of Agent, we can see that the <code class="language-plaintext highlighter-rouge">rbd</code> kernel module is not available in the current system, neither as a builtin nor a loadable external kernel module.</p>

<p>In this case, you have to <a href="https://www.linuxjournal.com/article/6568">re-configure and build</a> a new kernel to address this issue, there’re two options:</p>

<ul>
  <li>Re-configure your kernel to make sure the <code class="language-plaintext highlighter-rouge">CONFIG_BLK_DEV_RBD=y</code> in the <code class="language-plaintext highlighter-rouge">.config</code> file, then build the kernel.</li>
  <li>Re-configure your kernel to make sure the <code class="language-plaintext highlighter-rouge">CONFIG_BLK_DEV_RBD=m</code> in the <code class="language-plaintext highlighter-rouge">.config</code> file, then build the kernel.</li>
</ul>

<p>Rebooting the system to use the new kernel, this issue should be fixed: the Agent will be in normal <code class="language-plaintext highlighter-rouge">running</code> status if everything was done correctly.</p>

<h2 id="using-multiple-shared-filesystem-cephfs-is-attempted-on-a-kernel-version-older-than-47">Using multiple shared filesystem (CephFS) is attempted on a kernel version older than 4.7</h2>

<h3 id="symptoms-9">Symptoms</h3>

<ul>
  <li>More than one shared filesystem (CephFS) has been created in the cluster</li>
  <li>A pod attempts to mount any other shared filesystem besides the <strong>first</strong> one that was created</li>
  <li>The pod incorrectly gets the first filesystem mounted instead of the intended filesystem</li>
</ul>

<h3 id="solution-9">Solution</h3>

<p>The only solution to this problem is to upgrade your kernel to <code class="language-plaintext highlighter-rouge">4.7</code> or higher.
This is due to a mount flag added in the kernel version <code class="language-plaintext highlighter-rouge">4.7</code> which allows to chose the filesystem by name.</p>

<p>For additional info on the kernel version requirement for multiple shared filesystems (CephFS), see <a href="/docs/rook/v1.5/ceph-filesystem.html#kernel-version-requirement">Filesystem - Kernel version requirement</a>.</p>

<h2 id="set-debug-log-level-for-all-ceph-daemons">Set debug log level for all Ceph daemons</h2>

<p>You can set a given log level and apply it to all the Ceph daemons at the same time.
For this, make sure the toolbox pod is running, then determine the level you want (between 0 and 20).
You can find the list of all subsystems and their default values in <a href="https://docs.ceph.com/en/latest/rados/troubleshooting/log-and-debug/#ceph-subsystems">Ceph logging and debug official guide</a>. Be careful when increasing the level as it will produce very verbose logs.</p>

<p>Assuming you want a log level of 1, you will run:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl -n rook-ceph exec deploy/rook-ceph-tools -- set-ceph-debug-level 1
</span></code></pre></div></div>

<p>Output:</p>

<blockquote>
  <pre><code class="language-quote">ceph config set global debug_context 1
ceph config set global debug_lockdep 1
...
...
</code></pre>
</blockquote>

<p>Once you are done debugging, you can revert all the debug flag to their default value by running the following:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl -n rook-ceph exec deploy/rook-ceph-tools -- set-ceph-debug-level default
</span></code></pre></div></div>

<h2 id="activate-log-to-file-for-a-particular-ceph-daemon">Activate log to file for a particular Ceph daemon</h2>

<p>They are cases where looking at Kubernetes logs is not enough for diverse reasons, but just to name a few:</p>

<ul>
  <li>not everyone is familiar for Kubernetes logging and expects to find logs in traditional directories</li>
  <li>logs get eaten (buffer limit from the log engine) and thus not requestable from Kubernetes</li>
</ul>

<p>So for each daemon, <code class="language-plaintext highlighter-rouge">dataDirHostPath</code> is used to store logs, if logging is activated.
Rook will bindmount <code class="language-plaintext highlighter-rouge">dataDirHostPath</code> for every pod.
As of Ceph Nautilus 14.2.1, it is possible to enable logging for a particular daemon on the fly.
Let’s say you want to enable logging for <code class="language-plaintext highlighter-rouge">mon.a</code>, but only for this daemon.
Using the toolbox or from inside the operator run:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">ceph config set mon.a log_to_file true
</span></code></pre></div></div>

<p>This will activate logging on the filesystem, you will be able to find logs in <code class="language-plaintext highlighter-rouge">dataDirHostPath/$NAMESPACE/log</code>, so typically this would mean <code class="language-plaintext highlighter-rouge">/var/lib/rook/rook-ceph/log</code>.
You don’t need to restart the pod, the effect will be immediate.</p>

<p>To disable the logging on file, simply set <code class="language-plaintext highlighter-rouge">log_to_file</code> to <code class="language-plaintext highlighter-rouge">false</code>.</p>

<p>For Ceph Luminous/Mimic releases, <code class="language-plaintext highlighter-rouge">mon_cluster_log_file</code> and <code class="language-plaintext highlighter-rouge">cluster_log_file</code> can be set to
<code class="language-plaintext highlighter-rouge">/var/log/ceph/XXXX</code> in the config override ConfigMap to enable logging. See the (Advanced
Documentation)[Documentation/advanced-configuration.md#kubernetes] for information about how to use
the config override ConfigMap.</p>

<p>For Ceph Luminous/Mimic releases, <code class="language-plaintext highlighter-rouge">mon_cluster_log_file</code> and <code class="language-plaintext highlighter-rouge">cluster_log_file</code> can be set to <code class="language-plaintext highlighter-rouge">/var/log/ceph/XXXX</code> in the config override ConfigMap to enable logging. See the <a href="#custom-cephconf-settings">Advanced Documentation</a> for information about how to use the config override ConfigMap.</p>

<h2 id="flex-storage-class-versus-ceph-csi-storage-class">Flex storage class versus Ceph CSI storage class</h2>

<p>Since Rook 1.1, Ceph CSI has become stable and moving forward is the ultimate replacement over the Flex driver.
However, not all Flex storage classes are available through Ceph CSI since it’s basically catching up on features.
Ceph CSI in its 1.2 version (with Rook 1.1) does not support the Erasure coded pools storage class.</p>

<p>So, if you are looking at using such storage class you should enable the Flex driver by setting <code class="language-plaintext highlighter-rouge">ROOK_ENABLE_FLEX_DRIVER: true</code> in your <code class="language-plaintext highlighter-rouge">operator.yaml</code>.
Also, if you are in the need of specific features and wonder if CSI is capable of handling them, you should read <a href="https://github.com/ceph/ceph-csi#support-matrix">the ceph-csi support matrix</a>.</p>

<p>See also the <a href="/docs/rook/v1.5/ceph-csi-troubleshooting.html">CSI Troubleshooting Guide</a>.</p>

<h2 id="a-worker-node-using-rbd-devices-hangs-up">A worker node using RBD devices hangs up</h2>

<h3 id="symptoms-10">Symptoms</h3>

<ul>
  <li>There is no progress on I/O from/to one of RBD devices (<code class="language-plaintext highlighter-rouge">/dev/rbd*</code> or <code class="language-plaintext highlighter-rouge">/dev/nbd*</code>).</li>
  <li>After that, the whole worker node hangs up.</li>
</ul>

<h3 id="investigation-6">Investigation</h3>

<p>This happens when the following conditions are satisfied.</p>

<ul>
  <li>The problematic RBD device and the corresponding OSDs are co-located.</li>
  <li>There is an XFS filesystem on top of this device.</li>
</ul>

<p>In addition, when this problem happens, you can see the following messages in <code class="language-plaintext highlighter-rouge">dmesg</code>.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>dmesg
<span class="c">...
</span><span class="go">[51717.039319] INFO: task kworker/2:1:5938 blocked for more than 120 seconds.
</span><span class="gp">[51717.039361]       Not tainted 4.15.0-72-generic #</span>81-Ubuntu
<span class="gp">[51717.039388] "echo 0 &gt;</span><span class="w"> </span>/proc/sys/kernel/hung_task_timeout_secs<span class="s2">" disables this message.
</span><span class="c">...
</span></code></pre></div></div>

<p>It’s so-called <code class="language-plaintext highlighter-rouge">hung_task</code> problem and means that there is a deadlock in the kernel. For more detail, please refer to <a href="https://github.com/rook/rook/issues/3132#issuecomment-580508760">the corresponding issue comment</a>.</p>

<h3 id="solution-10">Solution</h3>

<p>This problem will be solve by the following two fixes.</p>

<ul>
  <li>Linux kernel: A minor feature that is introduced by <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=8d19f1c8e1937baf74e1962aae9f90fa3aeab463">this commit</a>. It will be included in Linux v5.6.</li>
  <li>Ceph: A fix that uses the above-mentioned kernel’s feature. The Ceph community will probably discuss this fix after releasing Linux v5.6.</li>
</ul>

<p>You can bypass this problem by using ext4 or any other filesystems rather than XFS. Filesystem type can be specified with <code class="language-plaintext highlighter-rouge">csi.storage.k8s.io/fstype</code> in StorageClass resource.</p>

<h2 id="too-few-pgs-per-osd-warning-is-shown">Too few PGs per OSD warning is shown</h2>

<h3 id="symptoms-11">Symptoms</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ceph status</code> shows “too few PGs per OSD” warning as follows.</li>
</ul>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>ceph status
<span class="go">  cluster:
    id:     fd06d7c3-5c5c-45ca-bdea-1cf26b783065
    health: HEALTH_WARN
            too few PGs per OSD (16 &lt; min 30)
</span></code></pre></div></div>

<h3 id="solution-11">Solution</h3>

<p>The meaning of this warning is written in <a href="https://docs.ceph.com/docs/master/rados/operations/health-checks#too-few-pgs">the document</a>.
However, in many cases it is benign. For more information, please see <a href="http://ceph.com/community/new-luminous-pg-overdose-protection/">the blog entry</a>.
Please refer to <a href="/docs/rook/v1.5/ceph-advanced-configuration.html#configuring-pools">Configuring Pools</a> if you want to know the proper <code class="language-plaintext highlighter-rouge">pg_num</code> of pools and change these values.</p>

<h2 id="lvm-metadata-can-be-corrupted-with-osd-on-lv-backed-pvc">LVM metadata can be corrupted with OSD on LV-backed PVC</h2>

<h3 id="symptoms-12">Symptoms</h3>

<p>There is a critical flaw in OSD on LV-backed PVC. LVM metadata can be corrupted if both the host and OSD container modify it simultaneously. For example, the administrator might modify it on the host, while the OSD initialization process in a container could modify it too. In addition, if <code class="language-plaintext highlighter-rouge">lvmetad</code> is running, the possibility of occurrence gets higher. In this case, the change of LVM metadata in OSD container is not reflected to LVM metadata cache in host for a while.</p>

<p>If you still decide to configure an OSD on LVM, please keep the following in mind to reduce the probability of this issue.</p>

<h3 id="solution-12">Solution</h3>

<ul>
  <li>Disable <code class="language-plaintext highlighter-rouge">lvmetad.</code></li>
  <li>Avoid configuration of LVs from the host. In addition, don’t touch the VGs and physical volumes that back these LVs.</li>
  <li>Avoid incrementing the <code class="language-plaintext highlighter-rouge">count</code> field of <code class="language-plaintext highlighter-rouge">storageClassDeviceSets</code> and create a new LV that backs an OSD simultaneously.</li>
</ul>

<p>You can know whether the above-mentioned tag exists with the command: <code class="language-plaintext highlighter-rouge">sudo lvs -o lv_name,lv_tags</code>. If the <code class="language-plaintext highlighter-rouge">lv_tag</code> field is empty in an LV corresponding to the OSD lv_tags, this OSD encountered the problem. In this case, please <a href="/docs/rook/v1.5/ceph-osd-mgmt.html#remove-an-osd">retire this OSD</a> or replace with other new OSD before restarting.</p>

<p>This problem doesn’t happen in newly created LV-backed PVCs because OSD container doesn’t modify LVM metadata anymore. The existing lvm mode OSDs work continuously even thought upgrade your Rook. However, using the raw mode OSDs is recommended because of the above-mentioned problem. You can replace the existing OSDs with raw mode OSDs by retiring them and adding new OSDs one by one. See the documents <a href="/docs/rook/v1.5/ceph-osd-mgmt.html#remove-an-osd">Remove an OSD</a> and <a href="/docs/rook/v1.5/ceph-osd-mgmt.html#add-an-osd-on-a-pvc">Add an OSD on a PVC</a>.</p>

<h2 id="osd-prepare-job-fails-due-to-low-aio-max-nr-setting">OSD prepare job fails due to low aio-max-nr setting</h2>

<p>If the Kernel is configured with a low <a href="https://www.kernel.org/doc/Documentation/sysctl/fs.txt">aio-max-nr setting</a>, the OSD prepare job might fail with the following error:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>exec: stderr: 2020-09-17T00:30:12.145+0000 7f0c17632f40 -1 bdev(0x56212de88700 /var/lib/ceph/osd/ceph-0//block) _aio_start io_setup(2) failed with EAGAIN; try increasing /proc/sys/fs/aio-max-nr
</code></pre></div></div>

<p>To overcome this, you need to increase the value of <code class="language-plaintext highlighter-rouge">fs.aio-max-nr</code> of your sysctl configuration (typically <code class="language-plaintext highlighter-rouge">/etc/sysctl.conf</code>).
You can do this with your favorite configuration management system.</p>

<p>Alternatively, you can have a <a href="https://github.com/rook/rook/issues/6279#issuecomment-694390514">DaemonSet</a> to apply the configuration for you on all your nodes.</p>

<h2 id="failed-to-create-crds">Failed to create CRDs</h2>
<p>If you are using Kubernetes version is v1.15 or older, you will see an error like this:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>unable to recognize "STDIN": no matches for kind "CustomResourceDefinition" in version "apiextensions.k8s.io/v1"
</code></pre></div></div>
<p>You need to create the CRDs found in <code class="language-plaintext highlighter-rouge">cluster/examples/kubernetes/ceph/pre-k8s-1.16</code>. Note that these pre-1.16 <code class="language-plaintext highlighter-rouge">apiextensions.k8s.io/v1beta1</code> CRDs are deprecated in k8s v1.16 and will no longer be supported from k8s v1.22.</p>

    </div>
  </div>
</div>

<script>
  var menu = [];
  var BASE_PATH = "";

  function add(name, url, isChild, current) {
    var item = { name: name, url: url, current: current };
    var container = menu;
    if (isChild && menu.length > 0) {
      menu[menu.length-1].children = menu[menu.length-1].children || [];
      container = menu[menu.length-1].children;
      if (current) {
        menu[menu.length-1].childCurrent = true;
      }
    }
    container.push(item);
  }

  
    add(
      "Rook",
      "/docs/rook/v1.5/",
      false,
      false
    );
  
    add(
      "Quickstart",
      "/docs/rook/v1.5/quickstart.html",
      false,
      false
    );
  
    add(
      "Cassandra",
      "/docs/rook/v1.5/cassandra.html",
      true,
      false
    );
  
    add(
      "Ceph Storage",
      "/docs/rook/v1.5/ceph-quickstart.html",
      true,
      false
    );
  
    add(
      "CockroachDB",
      "/docs/rook/v1.5/cockroachdb.html",
      true,
      false
    );
  
    add(
      "EdgeFS Data Fabric",
      "/docs/rook/v1.5/edgefs-quickstart.html",
      true,
      false
    );
  
    add(
      "Network Filesystem (NFS)",
      "/docs/rook/v1.5/nfs.html",
      true,
      false
    );
  
    add(
      "YugabyteDB",
      "/docs/rook/v1.5/yugabytedb.html",
      true,
      false
    );
  
    add(
      "Prerequisites",
      "/docs/rook/v1.5/k8s-pre-reqs.html",
      false,
      false
    );
  
    add(
      "FlexVolume Configuration",
      "/docs/rook/v1.5/flexvolume.html",
      true,
      false
    );
  
    add(
      "Ceph Storage",
      "/docs/rook/v1.5/ceph-storage.html",
      false,
      false
    );
  
    add(
      "Prerequisites",
      "/docs/rook/v1.5/ceph-prerequisites.html",
      true,
      false
    );
  
    add(
      "Admission Controller",
      "/docs/rook/v1.5/admission-controller-usage.html",
      true,
      false
    );
  
    add(
      "Examples",
      "/docs/rook/v1.5/ceph-examples.html",
      true,
      false
    );
  
    add(
      "OpenShift",
      "/docs/rook/v1.5/ceph-openshift.html",
      true,
      false
    );
  
    add(
      "Block Storage",
      "/docs/rook/v1.5/ceph-block.html",
      true,
      false
    );
  
    add(
      "Object Storage",
      "/docs/rook/v1.5/ceph-object.html",
      true,
      false
    );
  
    add(
      "Object Multisite",
      "/docs/rook/v1.5/ceph-object-multisite.html",
      true,
      false
    );
  
    add(
      "Shared Filesystem",
      "/docs/rook/v1.5/ceph-filesystem.html",
      true,
      false
    );
  
    add(
      "Ceph Dashboard",
      "/docs/rook/v1.5/ceph-dashboard.html",
      true,
      false
    );
  
    add(
      "Prometheus Monitoring",
      "/docs/rook/v1.5/ceph-monitoring.html",
      true,
      false
    );
  
    add(
      "Cluster CRD",
      "/docs/rook/v1.5/ceph-cluster-crd.html",
      true,
      false
    );
  
    add(
      "Block Pool CRD",
      "/docs/rook/v1.5/ceph-pool-crd.html",
      true,
      false
    );
  
    add(
      "Object Store CRD",
      "/docs/rook/v1.5/ceph-object-store-crd.html",
      true,
      false
    );
  
    add(
      "Object Multisite CRDs",
      "/docs/rook/v1.5/ceph-object-multisite-crd.html",
      true,
      false
    );
  
    add(
      "Object Bucket Claim",
      "/docs/rook/v1.5/ceph-object-bucket-claim.html",
      true,
      false
    );
  
    add(
      "Object Store User CRD",
      "/docs/rook/v1.5/ceph-object-store-user-crd.html",
      true,
      false
    );
  
    add(
      "Shared Filesystem CRD",
      "/docs/rook/v1.5/ceph-filesystem-crd.html",
      true,
      false
    );
  
    add(
      "NFS CRD",
      "/docs/rook/v1.5/ceph-nfs-crd.html",
      true,
      false
    );
  
    add(
      "Ceph CSI",
      "/docs/rook/v1.5/ceph-csi-drivers.html",
      true,
      false
    );
  
    add(
      "Volume clone",
      "/docs/rook/v1.5/ceph-csi-volume-clone.html",
      true,
      false
    );
  
    add(
      "Snapshots",
      "/docs/rook/v1.5/ceph-csi-snapshot.html",
      true,
      false
    );
  
    add(
      "RBDMirror CRD",
      "/docs/rook/v1.5/ceph-rbd-mirror-crd.html",
      true,
      false
    );
  
    add(
      "Client CRD",
      "/docs/rook/v1.5/ceph-client-crd.html",
      true,
      false
    );
  
    add(
      "Configuration",
      "/docs/rook/v1.5/ceph-configuration.html",
      true,
      false
    );
  
    add(
      "Upgrades",
      "/docs/rook/v1.5/ceph-upgrade.html",
      true,
      false
    );
  
    add(
      "Cleanup",
      "/docs/rook/v1.5/ceph-teardown.html",
      true,
      false
    );
  
    add(
      "EdgeFS Data Fabric",
      "/docs/rook/v1.5/edgefs-storage.html",
      false,
      false
    );
  
    add(
      "Cluster CRD",
      "/docs/rook/v1.5/edgefs-cluster-crd.html",
      true,
      false
    );
  
    add(
      "ISGW Link CRD",
      "/docs/rook/v1.5/edgefs-isgw-crd.html",
      true,
      false
    );
  
    add(
      "Scale-Out NFS CRD",
      "/docs/rook/v1.5/edgefs-nfs-crd.html",
      true,
      false
    );
  
    add(
      "Scale-Out SMB CRD",
      "/docs/rook/v1.5/edgefs-smb-crd.html",
      true,
      false
    );
  
    add(
      "Edge-X S3 CRD",
      "/docs/rook/v1.5/edgefs-s3x-crd.html",
      true,
      false
    );
  
    add(
      "AWS S3 CRD",
      "/docs/rook/v1.5/edgefs-s3-crd.html",
      true,
      false
    );
  
    add(
      "OpenStack/SWIFT CRD",
      "/docs/rook/v1.5/edgefs-swift-crd.html",
      true,
      false
    );
  
    add(
      "iSCSI Target CRD",
      "/docs/rook/v1.5/edgefs-iscsi-crd.html",
      true,
      false
    );
  
    add(
      "CSI driver",
      "/docs/rook/v1.5/edgefs-csi.html",
      true,
      false
    );
  
    add(
      "Monitoring",
      "/docs/rook/v1.5/edgefs-monitoring.html",
      true,
      false
    );
  
    add(
      "User Interface",
      "/docs/rook/v1.5/edgefs-ui.html",
      true,
      false
    );
  
    add(
      "VDEV Management",
      "/docs/rook/v1.5/edgefs-vdev-management.html",
      true,
      false
    );
  
    add(
      "Upgrade",
      "/docs/rook/v1.5/edgefs-upgrade.html",
      true,
      false
    );
  
    add(
      "Cassandra Cluster CRD",
      "/docs/rook/v1.5/cassandra-cluster-crd.html",
      false,
      false
    );
  
    add(
      "Upgrade",
      "/docs/rook/v1.5/cassandra-operator-upgrade.html",
      true,
      false
    );
  
    add(
      "CockroachDB Cluster CRD",
      "/docs/rook/v1.5/cockroachdb-cluster-crd.html",
      false,
      false
    );
  
    add(
      "NFS Server CRD",
      "/docs/rook/v1.5/nfs-crd.html",
      false,
      false
    );
  
    add(
      "YugabyteDB Cluster CRD",
      "/docs/rook/v1.5/yugabytedb-cluster-crd.html",
      false,
      false
    );
  
    add(
      "Helm Charts",
      "/docs/rook/v1.5/helm.html",
      false,
      false
    );
  
    add(
      "Ceph Operator",
      "/docs/rook/v1.5/helm-operator.html",
      true,
      false
    );
  
    add(
      "Common Issues",
      "/docs/rook/v1.5/common-issues.html",
      false,
      false
    );
  
    add(
      "Ceph Tools",
      "/docs/rook/v1.5/ceph-tools.html",
      false,
      false
    );
  
    add(
      "Toolbox",
      "/docs/rook/v1.5/ceph-toolbox.html",
      true,
      false
    );
  
    add(
      "Common Issues",
      "/docs/rook/v1.5/ceph-common-issues.html",
      true,
      true
    );
  
    add(
      "CSI Common Issues",
      "/docs/rook/v1.5/ceph-csi-troubleshooting.html",
      true,
      false
    );
  
    add(
      "Monitor Health",
      "/docs/rook/v1.5/ceph-mon-health.html",
      true,
      false
    );
  
    add(
      "OSD Management",
      "/docs/rook/v1.5/ceph-osd-mgmt.html",
      true,
      false
    );
  
    add(
      "Direct Tools",
      "/docs/rook/v1.5/direct-tools.html",
      true,
      false
    );
  
    add(
      "Advanced Configuration",
      "/docs/rook/v1.5/ceph-advanced-configuration.html",
      true,
      false
    );
  
    add(
      "OpenShift Common Issues",
      "/docs/rook/v1.5/ceph-openshift-issues.html",
      true,
      false
    );
  
    add(
      "Disaster Recovery",
      "/docs/rook/v1.5/ceph-disaster-recovery.html",
      true,
      false
    );
  
    add(
      "Tectonic Configuration",
      "/docs/rook/v1.5/tectonic.html",
      true,
      false
    );
  
    add(
      "Contributing",
      "/docs/rook/v1.5/development-flow.html",
      false,
      false
    );
  
    add(
      "Storage Providers",
      "/docs/rook/v1.5/storage-providers.html",
      true,
      false
    );
  
    add(
      "Multi-Node Test Environment",
      "/docs/rook/v1.5/development-environment.html",
      true,
      false
    );
  

  function getEntry(item) {
    var itemDom = document.createElement('li');

    if (item.current) {
      itemDom.innerHTML = item.name;
      itemDom.classList.add('current');
    } else {
      itemDom.innerHTML = '<a href="' + item.url + '">' + item.name + '</a>';
    }

    return itemDom;
  }

  // Flush css changes as explained in: https://stackoverflow.com/a/34726346
  // and more completely: https://stackoverflow.com/a/6956049
  function flushCss(element) {
    element.offsetHeight;
  }

  function addArrow(itemDom) {
    var MAIN_ITEM_HEIGHT = 24;
    var BOTTOM_PADDING = 20;
    var arrowDom = document.createElement('a');
    arrowDom.classList.add('arrow');
    arrowDom.innerHTML = '<img src="' + BASE_PATH + '/images/arrow.svg" />';
    arrowDom.onclick = function(itemDom) {
      return function () {
        // Calculated full height of the opened list
        var fullHeight = MAIN_ITEM_HEIGHT + BOTTOM_PADDING + itemDom.lastChild.clientHeight + 'px';

        itemDom.classList.toggle('open');

        if (itemDom.classList.contains('open')) {
          itemDom.style.height = fullHeight;
        } else {
          // If the list height is auto we have to set it to fullHeight
          // without tranistion before we shrink it to collapsed height
          if (itemDom.style.height === 'auto') {
            itemDom.style.transition = 'none';
            itemDom.style.height = fullHeight;
            flushCss(itemDom);
            itemDom.style.transition = '';
          }
          itemDom.style.height = MAIN_ITEM_HEIGHT + 'px';
        }

        return false;
      };
    }(itemDom);
    itemDom.appendChild(arrowDom);

    if ((item.current && item.children) || item.childCurrent) {
      itemDom.classList.add('open');
      itemDom.style.height = 'auto';
    } else {
      itemDom.style.height = MAIN_ITEM_HEIGHT + 'px';
    }
  }

  var menuDom = document.getElementById('docs-ul');
  for (var i = 0; i < menu.length; i++) {
    var item = menu[i];
    var itemDom = getEntry(item);

    if (item.childCurrent) {
      itemDom.classList.add('childCurrent');
    }

    if (item.children) {
      addArrow(itemDom);
      itemDom.classList.add('children');
      var children = document.createElement('ul');
      for (var j = 0; j < item.children.length; j++) {
        children.appendChild(getEntry(item.children[j]));
      }
      itemDom.appendChild(children);
    }
    menuDom.appendChild(itemDom);
  }
</script>
</div></main>
    <footer id="footer" aria-label="Footer">
  <div class="top">
    <a href="//www.cncf.io">
      <img
        class="cncf"
        src="/images/cncf.png"
        srcset="/images/cncf@2x.png 2x, /images/cncf@3x.png 3x" />
    </a>
    <p>We are a Cloud Native Computing Foundation graduated project.</p>
  </div>
  <div class="middle">
    <div class="grid-center">
      <div class="col_sm-12">
        <span>Getting Started</span>
        <a href="//github.com/rook/rook">GitHub</a>
        <a href="/docs/rook/v1.9/">Documentation</a>
        <a href="//github.com/rook/rook/blob/master/CONTRIBUTING.md#how-to-contribute">How to Contribute</a>
      </div>
      <div class="col_sm-12">
        <span>Community</span>
        <a href="//slack.rook.io/">Slack</a>
        <a href="//twitter.com/rook_io">Twitter</a>
        <a href="//groups.google.com/forum/#!forum/rook-dev">Forum</a>
        <a href="//blog.rook.io/">Blog</a>
      </div>
      <div class="col_sm-12">
        <span>Contact</span>
        <a href="mailto:cncf-rook-info@lists.cncf.io">Email</a>
        <a href="//github.com/rook/rook/issues">Feature request</a>
      </div>
      <div class="col_sm-12">
        <span>Top Contributors</span>
        <a href="//cloudical.io/">Cloudical</a>
        <a href="//cybozu.com">Cybozu, Inc</a>
        <a href="//www.redhat.com">Red Hat</a>
        <a href="//www.suse.com/">SUSE</a>
        <a href="//upbound.io">Upbound</a>
      </div>
    </div>
  </div>
  <div class="bottom">
    <div class="grid-center">
      <div class="col-8">
        <a class="logo" href="/">
          <img src="/images/rook-logo-small.svg" alt="rook.io" />
        </a>
        <p>
          &#169; Rook Authors 2022. Documentation distributed under
          <a href="https://creativecommons.org/licenses/by/4.0">CC-BY-4.0</a>.
        </p>
        <p>
          &#169; 2022 The Linux Foundation. All rights reserved. The Linux Foundation has
          registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our
          <a href="https://www.linuxfoundation.org/trademark-usage/">Trademark Usage</a> page.
        </p>
      </div>
    </div>
  </div>
</footer>


  <script src="/js/anchor.js"></script>
  <script>
    anchors.options = {
      placement: 'right',
      icon: '#',
    }

    document.addEventListener('DOMContentLoaded', function(event) {
      anchors.add('.docs-text h1, .docs-text h2, .docs-text h3, .docs-text h4, .docs-text h5, .docs-text h6');
    });
  </script>




    
  </body>
</html>
