












































<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />

    
    <meta name="robots" content="noindex">
    

    <title>Ceph Docs</title>

    <link rel="canonical" href="https://rook.io/docs/rook/v1.7/ceph-upgrade.html">

    <link rel="icon" href="/favicon.ico" />
<link rel="icon" type="image/png" href="/images/favicon_16x16.png" sizes="16x16" />
<link rel="icon" type="image/png" href="/images/favicon_32x32.png" sizes="32x32" />
<link rel="icon" type="image/png" href="/images/favicon_48x48.png" sizes="48x48" />
<link rel="icon" type="image/png" href="/images/favicon_192x192.png" sizes="192x192" />


    <link href="//fonts.googleapis.com/css?family=Montserrat:500|Open+Sans:300,400,600" rel="stylesheet">
    
    <link rel="stylesheet" href="/css/main.css">
    
      <link rel="stylesheet" href="/css/docs.css" />
    
  </head>
  <body>
    <nav id="navigation" aria-label="Navigation">
  <div>
    <div class="logo">
      <a href="/"><img src="/images/rook-logo.svg"/></a>
    </div>
    <div
      class="hamburger-controls"
      onclick="if (document.body.classList.contains('menu-open')) { document.body.classList.remove('menu-open') } else { document.body.classList.add('menu-open') }; return false;">
      <span></span> <span></span> <span></span>
    </div>
    <ul class="links">
      <li class="dropdown">
        <a role="button" href="javascript:void(0)">Documentation</a>
        <div class="dropdown-content">
          <a href="/docs/rook/v1.9/">Ceph</a>
          <a href="/docs/cassandra/v1.7/">Cassandra</a>
          <a href="/docs/nfs/v1.7/">NFS</a>
        </div>
      <li class="dropdown">
        <a role="button" href="javascript:void(0)">Community</a>
        <div class="dropdown-content">
          <a href="//github.com/rook/rook">GitHub</a>
          <a href="//slack.rook.io/">Slack</a>
          <a href="//groups.google.com/forum/#!forum/rook-dev">Forum</a>
          <a href="//twitter.com/rook_io">Twitter</a>
        </div>
      </li>
      <li><a href="//blog.rook.io/">Blog</a></li>
      <li><a class="button small" href="/docs/rook/v1.9/quickstart.html">Get Started</a></li>
    </ul>
  </div>
</nav>

    <main id="content" aria-label="Content"><div>



















<section class="docs-header">
  <h1>Ceph</h1>
  <div class="versions">
    <a role="button" href="javascript:void(0)">Rook Ceph v1.7</a>
    <div class="versions-dropdown-content">
      
        <a href="/docs/rook/v1.9/ceph-upgrade.html">Rook Ceph v1.9</a>
      
        <a href="/docs/rook/v1.8/ceph-upgrade.html">Rook Ceph v1.8</a>
      
        <a href="/docs/rook/v1.7/ceph-upgrade.html" class="active">Rook Ceph v1.7</a>
      
        <a href="/docs/rook/v1.6/ceph-upgrade.html">Rook Ceph v1.6</a>
      
        <a href="/docs/rook/v1.5/ceph-upgrade.html">Rook Ceph v1.5</a>
      
        <a href="/docs/rook/v1.4/ceph-upgrade.html">Rook Ceph v1.4</a>
      
        <a href="/docs/rook/v1.3/ceph-upgrade.html">Rook Ceph v1.3</a>
      
        <a href="/docs/rook/v1.2/ceph-upgrade.html">Rook Ceph v1.2</a>
      
        <a href="/docs/rook/v1.1/ceph-upgrade.html">Rook Ceph v1.1</a>
      
        <a href="/docs/rook/v1.0/ceph-upgrade.html">Rook Ceph v1.0</a>
      
        <a href="/docs/rook/v0.9/ceph-upgrade.html">Rook Ceph v0.9</a>
      
        <a href="/docs/rook/v0.8/ceph-upgrade.html">Rook Ceph v0.8</a>
      
        <a href="/docs/rook/v0.7/ceph-upgrade.html">Rook Ceph v0.7</a>
      
        <a href="/docs/rook/v0.6/ceph-upgrade.html">Rook Ceph v0.6</a>
      
        <a href="/docs/rook/v0.5/ceph-upgrade.html">Rook Ceph v0.5</a>
      
        <a href="/docs/rook/latest/ceph-upgrade.html">Rook Ceph latest</a>
      
    </div>
    <img src="/images/arrow.svg" />
  </div>
</section>
<div class="page">
  <div class="docs-menu">
      <ul id="docs-ul"></ul>
  </div>
  <div class="docs-content">
    <div class="docs-actions">
      <a id="edit" href="https://github.com/rook/rook/blob/master/Documentation/ceph-upgrade.md">Edit on GitHub</a>
    </div>
    
      <div class="alert old">
        <p><b>PLEASE NOTE</b>: This document applies to v1.7 version and not to the latest <strong>stable</strong> release v1.9</p>
      </div>
    
    <div class="docs-text">
      <h1 id="rook-ceph-upgrades">Rook-Ceph Upgrades</h1>

<p>This guide will walk you through the steps to upgrade the software in a Rook-Ceph cluster from one
version to the next. This includes both the Rook-Ceph operator software itself as well as the Ceph
cluster software.</p>

<p>Upgrades for both the operator and for Ceph are nearly entirely automated save for where Rook’s
permissions need to be explicitly updated by an admin or when incompatibilities need to be addressed
manually due to customizations.</p>

<p>We welcome feedback and opening issues!</p>

<h2 id="supported-versions">Supported Versions</h2>

<p>This guide is for upgrading from <strong>Rook v1.6.x to Rook v1.7.x</strong>.</p>

<p>Please refer to the upgrade guides from previous releases for supported upgrade paths.
Rook upgrades are only supported between official releases. Upgrades to and from <code class="language-plaintext highlighter-rouge">master</code> are not
supported.</p>

<p>For a guide to upgrade previous versions of Rook, please refer to the version of documentation for
those releases.</p>

<ul>
  <li><a href="https://rook.io/docs/rook/v1.6/ceph-upgrade.html">Upgrade 1.5 to 1.6</a></li>
  <li><a href="https://rook.io/docs/rook/v1.5/ceph-upgrade.html">Upgrade 1.4 to 1.5</a></li>
  <li><a href="https://rook.io/docs/rook/v1.4/ceph-upgrade.html">Upgrade 1.3 to 1.4</a></li>
  <li><a href="https://rook.io/docs/rook/v1.3/ceph-upgrade.html">Upgrade 1.2 to 1.3</a></li>
  <li><a href="https://rook.io/docs/rook/v1.2/ceph-upgrade.html">Upgrade 1.1 to 1.2</a></li>
  <li><a href="https://rook.io/docs/rook/v1.1/ceph-upgrade.html">Upgrade 1.0 to 1.1</a></li>
  <li><a href="https://rook.io/docs/rook/v1.0/ceph-upgrade.html">Upgrade 0.9 to 1.0</a></li>
  <li><a href="https://rook.io/docs/rook/v0.9/ceph-upgrade.html">Upgrade 0.8 to 0.9</a></li>
  <li><a href="https://rook.io/docs/rook/v0.8/upgrade.html">Upgrade 0.7 to 0.8</a></li>
  <li><a href="https://rook.io/docs/rook/v0.7/upgrade.html">Upgrade 0.6 to 0.7</a></li>
  <li><a href="https://rook.io/docs/rook/v0.6/upgrade.html">Upgrade 0.5 to 0.6</a></li>
</ul>

<h2 id="considerations">Considerations</h2>

<p>With this upgrade guide, there are a few notes to consider:</p>

<ul>
  <li><strong>WARNING</strong>: Upgrading a Rook cluster is not without risk. There may be unexpected issues or
obstacles that damage the integrity and health of your storage cluster, including data loss.</li>
  <li>The Rook cluster’s storage may be unavailable for short periods during the upgrade process for
both Rook operator updates and for Ceph version updates.</li>
  <li>We recommend that you read this document in full before you undertake a Rook cluster upgrade.</li>
</ul>

<h2 id="patch-release-upgrades">Patch Release Upgrades</h2>

<p>Unless otherwise noted due to extenuating requirements, upgrades from one patch release of Rook to
another are as simple as updating the common resources and the image of the Rook operator. For
example, when Rook v1.7.11 is released, the process of updating from v1.7.0 is as simple as running
the following:</p>

<p>First get the latest common resources manifests that contain the latest changes for Rook v1.7.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone <span class="nt">--single-branch</span> <span class="nt">--depth</span><span class="o">=</span>1 <span class="nt">--branch</span> v1.7.11 https://github.com/rook/rook.git
<span class="nb">cd </span>rook/cluster/examples/kubernetes/ceph
</code></pre></div></div>

<p><strong>IMPORTANT</strong> If you have RBD or CephFS volumes and are upgrading from Rook v1.6.0 - v1.6.4,
there is an issue upgrading from those versions that causes the volumes to hang.
Nodes will need to be restarted for the volumes to connect again. See
<a href="https://github.com/rook/rook/issues/8085#issuecomment-859234755">this issue</a> for more details.
Future upgrades of Rook will not have this issue.</p>

<p>If you have deployed the Rook Operator or the Ceph cluster into a different namespace than
<code class="language-plaintext highlighter-rouge">rook-ceph</code>, see the <a href="#1-update-common-resources-and-crds">Update common resources and CRDs</a>
section for instructions on how to change the default namespaces in <code class="language-plaintext highlighter-rouge">common.yaml</code>.</p>

<p>Then apply the latest changes from v1.7 and update the Rook Operator image.</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl apply -f common.yaml -f crds.yaml
kubectl -n rook-ceph set image deploy/rook-ceph-operator rook-ceph-operator=rook/ceph:v1.7.11
</span></code></pre></div></div>

<p>As exemplified above, it is a good practice to update Rook-Ceph common resources from the example
manifests before any update. The common resources and CRDs might not be updated with every
release, but K8s will only apply updates to the ones that changed.</p>

<p>Also update optional resources like Prometheus monitoring noted more fully in the
<a href="#updates-for-optional-resources">upgrade section below</a>.</p>

<h2 id="helm">Helm</h2>

<ul>
  <li>The minimum supported Helm version is <strong>v3.2.0</strong></li>
</ul>

<p>If you have installed Rook via the Helm chart, Helm will handle some details of the upgrade for you.
The upgrade steps in this guide will clarify if Helm manages the step for you.</p>

<p>The <code class="language-plaintext highlighter-rouge">rook-ceph</code> helm chart upgrade performs the Rook upgrade.
The <code class="language-plaintext highlighter-rouge">rook-ceph-cluster</code> helm chart upgrade performs a <a href="#ceph-version-upgrades">Ceph upgrade</a> if the Ceph image is updated.</p>

<h2 id="upgrading-from-v16-to-v17">Upgrading from v1.6 to v1.7</h2>

<p><strong>Rook releases from master are expressly unsupported.</strong> It is strongly recommended that you use
<a href="https://github.com/rook/rook/releases">official releases</a> of Rook. Unreleased versions from the
master branch are subject to changes and incompatibilities that will not be supported in the
official releases. Builds from the master branch can have functionality changed or removed at any
time without compatibility support and without prior notice.</p>

<h3 id="prerequisites"><strong>Prerequisites</strong></h3>

<p>We will do all our work in the Ceph example manifests directory.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cd</span> <span class="nv">$YOUR_ROOK_REPO</span>/cluster/examples/kubernetes/ceph/
</code></pre></div></div>

<p>Unless your Rook cluster was created with customized namespaces, namespaces for Rook clusters are
likely to be:</p>

<ul>
  <li>Clusters created by v0.7 or earlier: <code class="language-plaintext highlighter-rouge">rook-system</code> and <code class="language-plaintext highlighter-rouge">rook</code></li>
  <li>Clusters created in v0.8 or v0.9: <code class="language-plaintext highlighter-rouge">rook-ceph-system</code> and <code class="language-plaintext highlighter-rouge">rook-ceph</code></li>
  <li>Clusters created in v1.0 or newer: only <code class="language-plaintext highlighter-rouge">rook-ceph</code></li>
</ul>

<p>With this guide, we do our best not to assume the namespaces in your cluster. To make things as easy
as possible, modify and use the below snippet to configure your environment. We will use these
environment variables throughout this document.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Parameterize the environment</span>
<span class="nb">export </span><span class="nv">ROOK_OPERATOR_NAMESPACE</span><span class="o">=</span><span class="s2">"rook-ceph"</span>
<span class="nb">export </span><span class="nv">ROOK_CLUSTER_NAMESPACE</span><span class="o">=</span><span class="s2">"rook-ceph"</span>
</code></pre></div></div>

<p>In order to successfully upgrade a Rook cluster, the following prerequisites must be met:</p>

<ul>
  <li>The cluster should be in a healthy state with full functionality. Review the
<a href="#health-verification">health verification section</a> in order to verify your cluster is in a good
starting state.</li>
  <li>All pods consuming Rook storage should be created, running, and in a steady state. No Rook
persistent volumes should be in the act of being created or deleted.</li>
</ul>

<p><strong>IMPORTANT</strong> If you have RBD or CephFS volumes and are upgrading from Rook v1.6.0 - v1.6.4,
there is an issue upgrading from those versions that causes the volumes to hang.
Nodes will need to be restarted for the volumes to connect again. See
<a href="https://github.com/rook/rook/issues/8085#issuecomment-859234755">this issue</a> for more details.
Future upgrades of Rook will not have this issue.</p>

<h2 id="health-verification">Health Verification</h2>

<p>Before we begin the upgrade process, let’s first review some ways that you can verify the health of
your cluster, ensuring that the upgrade is going smoothly after each step. Most of the health
verification checks for your cluster during the upgrade process can be performed with the Rook
toolbox. For more information about how to run the toolbox, please visit the
<a href="/docs/rook/v1.7/ceph-toolbox.html">Rook toolbox readme</a>.</p>

<p>See the common issues pages for troubleshooting and correcting health issues:</p>

<ul>
  <li><a href="/docs/rook/v1.7/common-issues.html">General troubleshooting</a></li>
  <li><a href="/docs/rook/v1.7/ceph-common-issues.html">Ceph troubleshooting</a></li>
</ul>

<h3 id="pods-all-running"><strong>Pods all Running</strong></h3>

<p>In a healthy Rook cluster, the operator, the agents and all Rook namespace pods should be in the
<code class="language-plaintext highlighter-rouge">Running</code> state and have few, if any, pod restarts. To verify this, run the following commands:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl <span class="nt">-n</span> <span class="nv">$ROOK_CLUSTER_NAMESPACE</span> get pods
</code></pre></div></div>

<h3 id="status-output"><strong>Status Output</strong></h3>

<p>The Rook toolbox contains the Ceph tools that can give you status details of the cluster with the
<code class="language-plaintext highlighter-rouge">ceph status</code> command. Let’s look at an output sample and review some of the details:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">TOOLS_POD</span><span class="o">=</span><span class="si">$(</span>kubectl <span class="nt">-n</span> <span class="nv">$ROOK_CLUSTER_NAMESPACE</span> get pod <span class="nt">-l</span> <span class="s2">"app=rook-ceph-tools"</span> <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.items[0].metadata.name}'</span><span class="si">)</span>
kubectl <span class="nt">-n</span> <span class="nv">$ROOK_CLUSTER_NAMESPACE</span> <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$TOOLS_POD</span> <span class="nt">--</span> ceph status
</code></pre></div></div>

<blockquote>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> cluster:
   id:     a3f4d647-9538-4aff-9fd1-b845873c3fe9
   health: HEALTH_OK

 services:
   mon: 3 daemons, quorum b,c,a
   mgr: a(active)
   mds: myfs-1/1/1 up  {0=myfs-a=up:active}, 1 up:standby-replay
   osd: 6 osds: 6 up, 6 in
   rgw: 1 daemon active

 data:
   pools:   9 pools, 900 pgs
   objects: 67  objects, 11 KiB
   usage:   6.1 GiB used, 54 GiB / 60 GiB avail
   pgs:     900 active+clean

 io:
   client:   7.4 KiB/s rd, 681 B/s wr, 11 op/s rd, 4 op/s wr
   recovery: 164 B/s, 1 objects/s
</code></pre></div>  </div>
</blockquote>

<p>In the output above, note the following indications that the cluster is in a healthy state:</p>

<ul>
  <li>Cluster health: The overall cluster status is <code class="language-plaintext highlighter-rouge">HEALTH_OK</code> and there are no warning or error status
messages displayed.</li>
  <li>Monitors (mon):  All of the monitors are included in the <code class="language-plaintext highlighter-rouge">quorum</code> list.</li>
  <li>Manager (mgr): The Ceph manager is in the <code class="language-plaintext highlighter-rouge">active</code> state.</li>
  <li>OSDs (osd): All OSDs are <code class="language-plaintext highlighter-rouge">up</code> and <code class="language-plaintext highlighter-rouge">in</code>.</li>
  <li>Placement groups (pgs): All PGs are in the <code class="language-plaintext highlighter-rouge">active+clean</code> state.</li>
  <li>(If applicable) Ceph filesystem metadata server (mds): all MDSes are <code class="language-plaintext highlighter-rouge">active</code> for all filesystems</li>
  <li>(If applicable) Ceph object store RADOS gateways (rgw): all daemons are <code class="language-plaintext highlighter-rouge">active</code></li>
</ul>

<p>If your <code class="language-plaintext highlighter-rouge">ceph status</code> output has deviations from the general good health described above, there may
be an issue that needs to be investigated further. There are other commands you may run for more
details on the health of the system, such as <code class="language-plaintext highlighter-rouge">ceph osd status</code>. See the
<a href="https://docs.ceph.com/docs/master/rados/troubleshooting/">Ceph troubleshooting docs</a> for help.</p>

<p>Rook will prevent the upgrade of the Ceph daemons if the health is in a <code class="language-plaintext highlighter-rouge">HEALTH_ERR</code> state.
If you desired to proceed with the upgrade anyway, you will need to set either
<code class="language-plaintext highlighter-rouge">skipUpgradeChecks: true</code> or <code class="language-plaintext highlighter-rouge">continueUpgradeAfterChecksEvenIfNotHealthy: true</code>
as described in the <a href="https://rook.github.io/docs/rook/v1.7/ceph-cluster-crd.html#cluster-settings">cluster CR settings</a>.</p>

<h3 id="container-versions"><strong>Container Versions</strong></h3>

<p>The container version running in a specific pod in the Rook cluster can be verified in its pod spec
output. For example for the monitor pod <code class="language-plaintext highlighter-rouge">mon-b</code>, we can verify the container version it is running
with the below commands:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">POD_NAME</span><span class="o">=</span><span class="si">$(</span>kubectl <span class="nt">-n</span> <span class="nv">$ROOK_CLUSTER_NAMESPACE</span> get pod <span class="nt">-o</span> custom-columns<span class="o">=</span>name:.metadata.name <span class="nt">--no-headers</span> | <span class="nb">grep </span>rook-ceph-mon-b<span class="si">)</span>
kubectl <span class="nt">-n</span> <span class="nv">$ROOK_CLUSTER_NAMESPACE</span> get pod <span class="k">${</span><span class="nv">POD_NAME</span><span class="k">}</span> <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.containers[0].image}'</span>
</code></pre></div></div>

<p>The status and container versions for all Rook pods can be collected all at once with the following
commands:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl <span class="nt">-n</span> <span class="nv">$ROOK_OPERATOR_NAMESPACE</span> get pod <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{range .items[*]}{.metadata.name}{"\n\t"}{.status.phase}{"\t\t"}{.spec.containers[0].image}{"\t"}{.spec.initContainers[0]}{"\n"}{end}'</span> <span class="o">&amp;&amp;</span> <span class="se">\</span>
kubectl <span class="nt">-n</span> <span class="nv">$ROOK_CLUSTER_NAMESPACE</span> get pod <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{range .items[*]}{.metadata.name}{"\n\t"}{.status.phase}{"\t\t"}{.spec.containers[0].image}{"\t"}{.spec.initContainers[0].image}{"\n"}{end}'</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">rook-version</code> label exists on Ceph controller resources. For various resource controllers, a
summary of the resource controllers can be gained with the commands below. These will report the
requested, updated, and currently available replicas for various Rook-Ceph resources in addition to
the version of Rook for resources managed by the updated Rook-Ceph operator. Note that the operator
and toolbox deployments do not have a <code class="language-plaintext highlighter-rouge">rook-version</code> label set.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl <span class="nt">-n</span> <span class="nv">$ROOK_CLUSTER_NAMESPACE</span> get deployments <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{range .items[*]}{.metadata.name}{"  \treq/upd/avl: "}{.spec.replicas}{"/"}{.status.updatedReplicas}{"/"}{.status.readyReplicas}{"  \trook-version="}{.metadata.labels.rook-version}{"\n"}{end}'</span>

kubectl <span class="nt">-n</span> <span class="nv">$ROOK_CLUSTER_NAMESPACE</span> get <span class="nb">jobs</span> <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{range .items[*]}{.metadata.name}{"  \tsucceeded: "}{.status.succeeded}{"      \trook-version="}{.metadata.labels.rook-version}{"\n"}{end}'</span>
</code></pre></div></div>

<h3 id="rook-volume-health"><strong>Rook Volume Health</strong></h3>

<p>Any pod that is using a Rook volume should also remain healthy:</p>

<ul>
  <li>The pod should be in the <code class="language-plaintext highlighter-rouge">Running</code> state with few, if any, restarts</li>
  <li>There should be no errors in its logs</li>
  <li>The pod should still be able to read and write to the attached Rook volume.</li>
</ul>

<h2 id="rook-operator-upgrade-process">Rook Operator Upgrade Process</h2>

<p>In the examples given in this guide, we will be upgrading a live Rook cluster running <code class="language-plaintext highlighter-rouge">v1.6.8</code> to
the version <code class="language-plaintext highlighter-rouge">v1.7.11</code>. This upgrade should work from any official patch release of Rook v1.6 to any
official patch release of v1.7.</p>

<p><strong>Rook release from <code class="language-plaintext highlighter-rouge">master</code> are expressly unsupported.</strong> It is strongly recommended that you use
<a href="https://github.com/rook/rook/releases">official releases</a> of Rook. Unreleased versions from the
master branch are subject to changes and incompatibilities that will not be supported in the
official releases. Builds from the master branch can have functionality changed or removed at any
time without compatibility support and without prior notice.</p>

<p>These methods should work for any number of Rook-Ceph clusters and Rook Operators as long as you
parameterize the environment correctly. Merely repeat these steps for each Rook-Ceph cluster
(<code class="language-plaintext highlighter-rouge">ROOK_CLUSTER_NAMESPACE</code>), and be sure to update the <code class="language-plaintext highlighter-rouge">ROOK_OPERATOR_NAMESPACE</code> parameter each time
if applicable.</p>

<p>Let’s get started!</p>

<h3 id="1-update-common-resources-and-crds"><strong>1. Update common resources and CRDs</strong></h3>

<blockquote>
  <p>Automatically updated if you are upgrading via the helm chart</p>
</blockquote>

<p>First apply updates to Rook-Ceph common resources. This includes slightly modified privileges (RBAC)
needed by the Operator. Also update the Custom Resource Definitions (CRDs).</p>

<blockquote>
  <p><strong>IMPORTANT:</strong> If you are using Kubernetes version v1.15 or lower, you will need to manually
modify the <code class="language-plaintext highlighter-rouge">common.yaml</code> file to use
<code class="language-plaintext highlighter-rouge">rbac.authorization.k8s.io/v1beta1</code> instead of <code class="language-plaintext highlighter-rouge">rbac.authorization.k8s.io/v1</code>
You will also need to apply <code class="language-plaintext highlighter-rouge">pre-k8s-1.16/crds.yaml</code> instead of <code class="language-plaintext highlighter-rouge">crds.yaml</code>.</p>
</blockquote>

<p>First get the latest common resources manifests that contain the latest changes.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone <span class="nt">--single-branch</span> <span class="nt">--depth</span><span class="o">=</span>1 <span class="nt">--branch</span> v1.7.11 https://github.com/rook/rook.git
<span class="nb">cd </span>rook/cluster/examples/kubernetes/ceph
</code></pre></div></div>

<p>If you have deployed the Rook Operator or the Ceph cluster into a different namespace than
<code class="language-plaintext highlighter-rouge">rook-ceph</code>, update the common resource manifests to use your <code class="language-plaintext highlighter-rouge">ROOK_OPERATOR_NAMESPACE</code> and
<code class="language-plaintext highlighter-rouge">ROOK_CLUSTER_NAMESPACE</code> using <code class="language-plaintext highlighter-rouge">sed</code>.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sed</span> <span class="nt">-i</span>.bak <span class="se">\</span>
    <span class="nt">-e</span> <span class="s2">"s/</span><span class="se">\(</span><span class="s2">.*</span><span class="se">\)</span><span class="s2">:.*# namespace:operator/</span><span class="se">\1</span><span class="s2">: </span><span class="nv">$ROOK_OPERATOR_NAMESPACE</span><span class="s2"> # namespace:operator/g"</span> <span class="se">\</span>
    <span class="nt">-e</span> <span class="s2">"s/</span><span class="se">\(</span><span class="s2">.*</span><span class="se">\)</span><span class="s2">:.*# namespace:cluster/</span><span class="se">\1</span><span class="s2">: </span><span class="nv">$ROOK_CLUSTER_NAMESPACE</span><span class="s2"> # namespace:cluster/g"</span> <span class="se">\</span>
  common.yaml
</code></pre></div></div>

<p>Then apply the latest changes.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-f</span> common.yaml <span class="nt">-f</span> crds.yaml
</code></pre></div></div>

<h4 id="updates-for-optional-resources"><strong>Updates for optional resources</strong></h4>

<p>If you have <a href="/docs/rook/v1.7/ceph-monitoring.html">Prometheus monitoring</a> enabled, follow the
step to upgrade the Prometheus RBAC resources as well.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-f</span> cluster/examples/kubernetes/ceph/monitoring/rbac.yaml
</code></pre></div></div>

<h3 id="2-update-ceph-csi-versions"><strong>2. Update Ceph CSI versions</strong></h3>

<blockquote>
  <p>Automatically updated if you are upgrading via the helm chart</p>
</blockquote>

<p>If you have specified custom CSI images in the Rook-Ceph Operator deployment, we recommended you
update to use the latest Ceph-CSI drivers. See the <a href="#csi-version">CSI Version</a> section for more
details.</p>

<blockquote>
  <p>Note: If using snapshots, refer to the <a href="/docs/rook/v1.7/ceph-csi-snapshot.html#upgrade-snapshot-api">Upgrade Snapshot API guide</a>.</p>
</blockquote>

<h3 id="3-update-the-rook-operator"><strong>3. Update the Rook Operator</strong></h3>

<blockquote>
  <p>Automatically updated if you are upgrading via the helm chart</p>
</blockquote>

<p>The largest portion of the upgrade is triggered when the operator’s image is updated to <code class="language-plaintext highlighter-rouge">v1.7.x</code>.
When the operator is updated, it will proceed to update all of the Ceph daemons.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl <span class="nt">-n</span> <span class="nv">$ROOK_OPERATOR_NAMESPACE</span> <span class="nb">set </span>image deploy/rook-ceph-operator rook-ceph-operator<span class="o">=</span>rook/ceph:v1.7.11
</code></pre></div></div>

<h3 id="4-wait-for-the-upgrade-to-complete"><strong>4. Wait for the upgrade to complete</strong></h3>

<p>Watch now in amazement as the Ceph mons, mgrs, OSDs, rbd-mirrors, MDSes and RGWs are terminated and
replaced with updated versions in sequence. The cluster may be offline very briefly as mons update,
and the Ceph Filesystem may fall offline a few times while the MDSes are upgrading. This is normal.</p>

<p>The versions of the components can be viewed as they are updated:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>watch <span class="nt">--exec</span> kubectl <span class="nt">-n</span> <span class="nv">$ROOK_CLUSTER_NAMESPACE</span> get deployments <span class="nt">-l</span> <span class="nv">rook_cluster</span><span class="o">=</span><span class="nv">$ROOK_CLUSTER_NAMESPACE</span> <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{range .items[*]}{.metadata.name}{"  \treq/upd/avl: "}{.spec.replicas}{"/"}{.status.updatedReplicas}{"/"}{.status.readyReplicas}{"  \trook-version="}{.metadata.labels.rook-version}{"\n"}{end}'</span>
</code></pre></div></div>

<p>As an example, this cluster is midway through updating the OSDs. When all deployments report <code class="language-plaintext highlighter-rouge">1/1/1</code>
availability and <code class="language-plaintext highlighter-rouge">rook-version=v1.7.11</code>, the Ceph cluster’s core components are fully updated.</p>

<blockquote>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Every 2.0s: kubectl -n rook-ceph get deployment -o j...

rook-ceph-mgr-a         req/upd/avl: 1/1/1      rook-version=v1.7.11
rook-ceph-mon-a         req/upd/avl: 1/1/1      rook-version=v1.7.11
rook-ceph-mon-b         req/upd/avl: 1/1/1      rook-version=v1.7.11
rook-ceph-mon-c         req/upd/avl: 1/1/1      rook-version=v1.7.11
rook-ceph-osd-0         req/upd/avl: 1//        rook-version=v1.7.11
rook-ceph-osd-1         req/upd/avl: 1/1/1      rook-version=v1.6.8
rook-ceph-osd-2         req/upd/avl: 1/1/1      rook-version=v1.6.8
</code></pre></div>  </div>
</blockquote>

<p>An easy check to see if the upgrade is totally finished is to check that there is only one
<code class="language-plaintext highlighter-rouge">rook-version</code> reported across the cluster.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>kubectl <span class="nt">-n</span> <span class="nv">$ROOK_CLUSTER_NAMESPACE</span> get deployment <span class="nt">-l</span> <span class="nv">rook_cluster</span><span class="o">=</span><span class="nv">$ROOK_CLUSTER_NAMESPACE</span> <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{range .items[*]}{"rook-version="}{.metadata.labels.rook-version}{"\n"}{end}'</span> | <span class="nb">sort</span> | <span class="nb">uniq</span>
<span class="go">This cluster is not yet finished:
  rook-version=v1.6.8
  rook-version=v1.7.11
This cluster is finished:
  rook-version=v1.7.11
</span></code></pre></div></div>

<h3 id="5-verify-the-updated-cluster"><strong>5. Verify the updated cluster</strong></h3>

<p>At this point, your Rook operator should be running version <code class="language-plaintext highlighter-rouge">rook/ceph:v1.7.11</code>.</p>

<p>Verify the Ceph cluster’s health using the <a href="#health-verification">health verification section</a>.</p>

<h3 id="6-update-cephrbdmirror-and-cephblockpool-configs"><strong>6. Update CephRBDMirror and CephBlockPool configs</strong></h3>

<p>If you are not using a <code class="language-plaintext highlighter-rouge">CephRBDMirror</code> in your Rook cluster, you may disregard this section.</p>

<p>Otherwise, please note that the location of the <code class="language-plaintext highlighter-rouge">CephRBDMirror</code> <code class="language-plaintext highlighter-rouge">spec.peers</code> config has moved to
<code class="language-plaintext highlighter-rouge">CephBlockPool</code> <code class="language-plaintext highlighter-rouge">spec.mirroring.peers</code> in Rook v1.7. This change allows each pool to have its own
peer and enables pools to re-use an existing peer secret if it points to the same cluster peer.</p>

<p>You may wish to see the <a href="/docs/rook/v1.7/ceph-pool-crd.html#spec">CephBlockPool spec Documentation</a> for the latest
configuration advice.</p>

<p>The pre-existing config location in <code class="language-plaintext highlighter-rouge">CephRBDMirror</code> <code class="language-plaintext highlighter-rouge">spec.peers</code> will continue to be supported, but
users are still encouraged to migrate this setting from <code class="language-plaintext highlighter-rouge">CephRBDMirror</code> to relevant <code class="language-plaintext highlighter-rouge">CephBlockPool</code>
resources.</p>

<p>To migrate the setting, follow these steps:</p>
<ol>
  <li>Stop the Rook-Ceph operator by downscaling the Deployment to zero replicas.
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl <span class="nt">-n</span> <span class="nv">$ROOK_OPERATOR_NAMESPACE</span> scale deployment rook-ceph-operator <span class="nt">--replicas</span><span class="o">=</span>0
</code></pre></div>    </div>
  </li>
  <li>Copy the <code class="language-plaintext highlighter-rouge">spec.peers</code> config from <code class="language-plaintext highlighter-rouge">CephRBDMirror</code> to every <code class="language-plaintext highlighter-rouge">CephBlockPool</code> in your cluster that
has mirroring enabled.</li>
  <li>Remove the <code class="language-plaintext highlighter-rouge">peers</code> spec from the <code class="language-plaintext highlighter-rouge">CephRBDMirror</code> resource.</li>
  <li>Resume the Rook-Ceph operator by scaling the Deployment back to one replica.
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl <span class="nt">-n</span> <span class="nv">$ROOK_OPERATOR_NAMESPACE</span> scale deployment rook-ceph-operator <span class="nt">--replicas</span><span class="o">=</span>1
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="ceph-version-upgrades">Ceph Version Upgrades</h2>

<p>Rook v1.7 supports the following Ceph versions:</p>
<ul>
  <li>Ceph Pacific 16.2.0 or newer</li>
  <li>Ceph Octopus v15.2.0 or newer</li>
  <li>Ceph Nautilus 14.2.5 or newer</li>
</ul>

<p>These are the only supported versions of Ceph. Rook v1.8 will no longer support Ceph Nautilus
(14.2.x), and users will have to upgrade Ceph to Octopus (15.2.x) or Pacific (16.2.x) upgrading to
Rook v1.8.</p>

<blockquote>
  <p><strong>IMPORTANT: When an update is requested, the operator will check Ceph’s status, if it is in <code class="language-plaintext highlighter-rouge">HEALTH_ERR</code> it will refuse to do the upgrade.</strong></p>
</blockquote>

<p>Rook is cautious when performing upgrades. When an upgrade is requested (the Ceph image has been
updated in the CR), Rook will go through all the daemons one by one and will individually perform
checks on them. It will make sure a particular daemon can be stopped before performing the upgrade.
Once the deployment has been updated, it checks if this is ok to continue. After each daemon is
updated we wait for things to settle (monitors to be in a quorum, PGs to be clean for OSDs, up for
MDSes, etc.), then only when the condition is met we move to the next daemon. We repeat this process
until all the daemons have been updated.</p>

<h3 id="disable-bluestore_fsck_quick_fix_on_mount">Disable <code class="language-plaintext highlighter-rouge">bluestore_fsck_quick_fix_on_mount</code></h3>
<blockquote>
  <p><strong>WARNING: There is a notice from Ceph for users upgrading to Ceph Pacific v16.2.6 or lower from
an earlier major version of Ceph. If you are upgrading to Ceph Pacific (v16), please upgrade to
v16.2.7 or higher if possible.</strong></p>
</blockquote>

<p>If you must upgrade to a version lower than v16.2.7, ensure that all instances of
<code class="language-plaintext highlighter-rouge">bluestore_fsck_quick_fix_on_mount</code> in Rook-Ceph configs are removed.</p>

<p>First, Ensure no references to <code class="language-plaintext highlighter-rouge">bluestore_fsck_quick_fix_on_mount</code> are present in the
<code class="language-plaintext highlighter-rouge">rook-config-override</code> <a href="/docs/rook/v1.7/ceph-advanced-configuration.html#custom-cephconf-settings">ConfigMap</a>. Remove
them if they exist.</p>

<p>Finally, ensure no references to <code class="language-plaintext highlighter-rouge">bluestore_fsck_quick_fix_on_mount</code> are present in Ceph’s internal
configuration. Run all commands below from the <a href="/docs/rook/v1.7/ceph-toolbox.html">toolbox</a>.</p>

<p>In the example below, two instances of <code class="language-plaintext highlighter-rouge">bluestore_fsck_quick_fix_on_mount</code> are present and are
commented, and some output text has been removed for brevity.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ceph config-key dump
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
    "config/global/bluestore_fsck_quick_fix_on_mount": "false",       # &lt;-- FALSE
    "config/global/osd_scrub_auto_repair": "true",
    "config/mgr.a/mgr/dashboard/server_port": "7000",
    "config/mgr/mgr/balancer/active": "true",
    "config/osd/bluestore_fsck_quick_fix_on_mount": "true",           # &lt;-- TRUE
}
</code></pre></div></div>

<p>Remove the configs for both with the commands below. Note how the <code class="language-plaintext highlighter-rouge">config/...</code> paths correspond to
the output above.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ceph config-key <span class="nb">rm </span>config/global/bluestore_fsck_quick_fix_on_mount
ceph config-key <span class="nb">rm </span>config/osd/bluestore_fsck_quick_fix_on_mount
</code></pre></div></div>

<p>It’s best to run <code class="language-plaintext highlighter-rouge">ceph config-key dump</code> again to verify references to
<code class="language-plaintext highlighter-rouge">bluestore_fsck_quick_fix_on_mount</code> are gone after this.</p>

<p>See for more information, see here: https://github.com/rook/rook/issues/9185</p>

<h3 id="ceph-images"><strong>Ceph images</strong></h3>

<p>Official Ceph container images can be found on <a href="https://quay.io/repository/ceph/ceph?tab=tags">Quay</a>.
Prior to August 2021, official images were on docker.io. While those images will remain on Docker Hub, all new images are being pushed to Quay.</p>

<p>These images are tagged in a few ways:</p>

<ul>
  <li>The most explicit form of tags are full-ceph-version-and-build tags (e.g., <code class="language-plaintext highlighter-rouge">v16.2.6-20210918</code>).
These tags are recommended for production clusters, as there is no possibility for the cluster to
be heterogeneous with respect to the version of Ceph running in containers.</li>
  <li>Ceph major version tags (e.g., <code class="language-plaintext highlighter-rouge">v16</code>) are useful for development and test clusters so that the
latest version of Ceph is always available.</li>
</ul>

<p><strong>Ceph containers other than the official images from the registry above will not be supported.</strong></p>

<h3 id="example-upgrade-to-ceph-pacific"><strong>Example upgrade to Ceph Pacific</strong></h3>

<h4 id="1-update-the-main-ceph-daemons"><strong>1. Update the main Ceph daemons</strong></h4>

<p>The majority of the upgrade will be handled by the Rook operator. Begin the upgrade by changing the
Ceph image field in the cluster CRD (<code class="language-plaintext highlighter-rouge">spec.cephVersion.image</code>).</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">NEW_CEPH_IMAGE</span><span class="o">=</span><span class="s1">'quay.io/ceph/ceph:v16.2.6-20210918'</span>
<span class="nv">CLUSTER_NAME</span><span class="o">=</span><span class="s2">"</span><span class="nv">$ROOK_CLUSTER_NAMESPACE</span><span class="s2">"</span>  <span class="c"># change if your cluster name is not the Rook namespace</span>
kubectl <span class="nt">-n</span> <span class="nv">$ROOK_CLUSTER_NAMESPACE</span> patch CephCluster <span class="nv">$CLUSTER_NAME</span> <span class="nt">--type</span><span class="o">=</span>merge <span class="nt">-p</span> <span class="s2">"{</span><span class="se">\"</span><span class="s2">spec</span><span class="se">\"</span><span class="s2">: {</span><span class="se">\"</span><span class="s2">cephVersion</span><span class="se">\"</span><span class="s2">: {</span><span class="se">\"</span><span class="s2">image</span><span class="se">\"</span><span class="s2">: </span><span class="se">\"</span><span class="nv">$NEW_CEPH_IMAGE</span><span class="se">\"</span><span class="s2">}}}"</span>
</code></pre></div></div>

<h4 id="2-wait-for-the-daemon-pod-updates-to-complete"><strong>2. Wait for the daemon pod updates to complete</strong></h4>

<p>As with upgrading Rook, you must now wait for the upgrade to complete. Status can be determined in a
similar way to the Rook upgrade as well.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>watch <span class="nt">--exec</span> kubectl <span class="nt">-n</span> <span class="nv">$ROOK_CLUSTER_NAMESPACE</span> get deployments <span class="nt">-l</span> <span class="nv">rook_cluster</span><span class="o">=</span><span class="nv">$ROOK_CLUSTER_NAMESPACE</span> <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{range .items[*]}{.metadata.name}{"  \treq/upd/avl: "}{.spec.replicas}{"/"}{.status.updatedReplicas}{"/"}{.status.readyReplicas}{"  \tceph-version="}{.metadata.labels.ceph-version}{"\n"}{end}'</span>
</code></pre></div></div>

<p>Determining when the Ceph has fully updated is rather simple.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">kubectl -n $</span>ROOK_CLUSTER_NAMESPACE get deployment <span class="nt">-l</span> <span class="nv">rook_cluster</span><span class="o">=</span><span class="nv">$ROOK_CLUSTER_NAMESPACE</span> <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{range .items[*]}{"ceph-version="}{.metadata.labels.ceph-version}{"\n"}{end}'</span> | <span class="nb">sort</span> | <span class="nb">uniq</span>
<span class="go">This cluster is not yet finished:
    ceph-version=15.2.13-0
    ceph-version=16.2.6-0
This cluster is finished:
    ceph-version=16.2.6-0
</span></code></pre></div></div>

<h4 id="3-verify-the-updated-cluster"><strong>3. Verify the updated cluster</strong></h4>

<p>Verify the Ceph cluster’s health using the <a href="#health-verification">health verification section</a>.</p>

<h2 id="csi-version">CSI Version</h2>

<p>If you have a cluster running with CSI drivers enabled and you want to configure Rook
to use non-default CSI images, the following settings will need to be applied for the desired
version of CSI.</p>

<p>The operator configuration variables have recently moved from the operator deployment to the
<code class="language-plaintext highlighter-rouge">rook-ceph-operator-config</code> ConfigMap. The values in the operator deployment can still be set,
but if the ConfigMap settings are applied, they will override the operator deployment settings.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">kubectl -n $</span>ROOK_OPERATOR_NAMESPACE edit configmap rook-ceph-operator-config
</code></pre></div></div>

<p>The default upstream images are included below, which you can change to your desired images.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">ROOK_CSI_CEPH_IMAGE</span><span class="pi">:</span> <span class="s2">"</span><span class="s">quay.io/cephcsi/cephcsi:v3.4.0"</span>
<span class="na">ROOK_CSI_REGISTRAR_IMAGE</span><span class="pi">:</span> <span class="s2">"</span><span class="s">k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.3.0"</span>
<span class="na">ROOK_CSI_PROVISIONER_IMAGE</span><span class="pi">:</span> <span class="s2">"</span><span class="s">k8s.gcr.io/sig-storage/csi-provisioner:v3.0.0"</span>
<span class="na">ROOK_CSI_ATTACHER_IMAGE</span><span class="pi">:</span> <span class="s2">"</span><span class="s">k8s.gcr.io/sig-storage/csi-attacher:v3.3.0"</span>
<span class="na">ROOK_CSI_RESIZER_IMAGE</span><span class="pi">:</span> <span class="s2">"</span><span class="s">k8s.gcr.io/sig-storage/csi-resizer:v1.3.0"</span>
<span class="na">ROOK_CSI_SNAPSHOTTER_IMAGE</span><span class="pi">:</span> <span class="s2">"</span><span class="s">k8s.gcr.io/sig-storage/csi-snapshotter:v4.2.0"</span>
<span class="na">CSI_VOLUME_REPLICATION_IMAGE</span><span class="pi">:</span> <span class="s2">"</span><span class="s">quay.io/csiaddons/volumereplication-operator:v0.1.0"</span>
</code></pre></div></div>

<h3 id="use-default-images"><strong>Use default images</strong></h3>

<p>If you would like Rook to use the inbuilt default upstream images, then you may simply remove all
variables matching <code class="language-plaintext highlighter-rouge">ROOK_CSI_*_IMAGE</code> from the above ConfigMap and/or the operator deployment.</p>

<h3 id="verifying-updates"><strong>Verifying updates</strong></h3>

<p>You can use the below command to see the CSI images currently being used in the cluster. Note that
not all images (like <code class="language-plaintext highlighter-rouge">volumereplication-operator</code>) may be present in every cluster depending on
which CSI features are enabled.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl --namespace rook-ceph get pod -o jsonpath='{range .items[*]}{range .spec.containers[*]}{.image}{"\n"}' -l 'app in (csi-rbdplugin,csi-rbdplugin-provisioner,csi-cephfsplugin,csi-cephfsplugin-provisioner)' | sort | uniq
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s.gcr.io/sig-storage/csi-attacher:v3.3.0
k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.3.0
k8s.gcr.io/sig-storage/csi-provisioner:v3.0.0
k8s.gcr.io/sig-storage/csi-resizer:v1.3.0
k8s.gcr.io/sig-storage/csi-snapshotter:v4.2.0
quay.io/cephcsi/cephcsi:v3.4.0
quay.io/csiaddons/volumereplication-operator:v0.1.0
</code></pre></div></div>

    </div>
  </div>
</div>

<script>
  var menu = [];
  var BASE_PATH = "";

  function add(name, url, isChild, current) {
    var item = { name: name, url: url, current: current };
    var container = menu;
    if (isChild && menu.length > 0) {
      menu[menu.length-1].children = menu[menu.length-1].children || [];
      container = menu[menu.length-1].children;
      if (current) {
        menu[menu.length-1].childCurrent = true;
      }
    }
    container.push(item);
  }

  
    add(
      "Rook",
      "/docs/rook/v1.7/",
      false,
      false
    );
  
    add(
      "Quickstart",
      "/docs/rook/v1.7/quickstart.html",
      false,
      false
    );
  
    add(
      "Prerequisites",
      "/docs/rook/v1.7/pre-reqs.html",
      false,
      false
    );
  
    add(
      "Authenticated Registries",
      "/docs/rook/v1.7/authenticated-registry.html",
      true,
      false
    );
  
    add(
      "FlexVolume Configuration",
      "/docs/rook/v1.7/flexvolume.html",
      true,
      false
    );
  
    add(
      "Pod Security Policies",
      "/docs/rook/v1.7/pod-security-policies.html",
      true,
      false
    );
  
    add(
      "Ceph Storage",
      "/docs/rook/v1.7/ceph-storage.html",
      false,
      false
    );
  
    add(
      "Admission Controller",
      "/docs/rook/v1.7/admission-controller-usage.html",
      true,
      false
    );
  
    add(
      "Examples",
      "/docs/rook/v1.7/ceph-examples.html",
      true,
      false
    );
  
    add(
      "OpenShift",
      "/docs/rook/v1.7/ceph-openshift.html",
      true,
      false
    );
  
    add(
      "Block Storage",
      "/docs/rook/v1.7/ceph-block.html",
      true,
      false
    );
  
    add(
      "Object Storage",
      "/docs/rook/v1.7/ceph-object.html",
      true,
      false
    );
  
    add(
      "Object Multisite",
      "/docs/rook/v1.7/ceph-object-multisite.html",
      true,
      false
    );
  
    add(
      "Shared Filesystem",
      "/docs/rook/v1.7/ceph-filesystem.html",
      true,
      false
    );
  
    add(
      "Ceph Dashboard",
      "/docs/rook/v1.7/ceph-dashboard.html",
      true,
      false
    );
  
    add(
      "Prometheus Monitoring",
      "/docs/rook/v1.7/ceph-monitoring.html",
      true,
      false
    );
  
    add(
      "Cluster CRD",
      "/docs/rook/v1.7/ceph-cluster-crd.html",
      true,
      false
    );
  
    add(
      "Block Pool CRD",
      "/docs/rook/v1.7/ceph-pool-crd.html",
      true,
      false
    );
  
    add(
      "Object Store CRD",
      "/docs/rook/v1.7/ceph-object-store-crd.html",
      true,
      false
    );
  
    add(
      "Object Multisite CRDs",
      "/docs/rook/v1.7/ceph-object-multisite-crd.html",
      true,
      false
    );
  
    add(
      "Object Bucket Claim",
      "/docs/rook/v1.7/ceph-object-bucket-claim.html",
      true,
      false
    );
  
    add(
      "Object Store User CRD",
      "/docs/rook/v1.7/ceph-object-store-user-crd.html",
      true,
      false
    );
  
    add(
      "Shared Filesystem CRD",
      "/docs/rook/v1.7/ceph-filesystem-crd.html",
      true,
      false
    );
  
    add(
      "NFS CRD",
      "/docs/rook/v1.7/ceph-nfs-crd.html",
      true,
      false
    );
  
    add(
      "Ceph CSI",
      "/docs/rook/v1.7/ceph-csi-drivers.html",
      true,
      false
    );
  
    add(
      "RBD Mirroring",
      "/docs/rook/v1.7/rbd-mirroring.html",
      true,
      false
    );
  
    add(
      "Failover and Failback",
      "/docs/rook/v1.7/async-disaster-recovery.html",
      true,
      false
    );
  
    add(
      "Volume clone",
      "/docs/rook/v1.7/ceph-csi-volume-clone.html",
      true,
      false
    );
  
    add(
      "Snapshots",
      "/docs/rook/v1.7/ceph-csi-snapshot.html",
      true,
      false
    );
  
    add(
      "Client CRD",
      "/docs/rook/v1.7/ceph-client-crd.html",
      true,
      false
    );
  
    add(
      "RBDMirror CRD",
      "/docs/rook/v1.7/ceph-rbd-mirror-crd.html",
      true,
      false
    );
  
    add(
      "FilesystemMirror CRD",
      "/docs/rook/v1.7/ceph-fs-mirror-crd.html",
      true,
      false
    );
  
    add(
      "Configuration",
      "/docs/rook/v1.7/ceph-configuration.html",
      true,
      false
    );
  
    add(
      "Upgrades",
      "/docs/rook/v1.7/ceph-upgrade.html",
      true,
      true
    );
  
    add(
      "Cleanup",
      "/docs/rook/v1.7/ceph-teardown.html",
      true,
      false
    );
  
    add(
      "Helm Charts",
      "/docs/rook/v1.7/helm.html",
      false,
      false
    );
  
    add(
      "Ceph Operator",
      "/docs/rook/v1.7/helm-operator.html",
      true,
      false
    );
  
    add(
      "Ceph Cluster",
      "/docs/rook/v1.7/helm-ceph-cluster.html",
      true,
      false
    );
  
    add(
      "Common Issues",
      "/docs/rook/v1.7/common-issues.html",
      false,
      false
    );
  
    add(
      "Ceph Tools",
      "/docs/rook/v1.7/ceph-tools.html",
      false,
      false
    );
  
    add(
      "Toolbox",
      "/docs/rook/v1.7/ceph-toolbox.html",
      true,
      false
    );
  
    add(
      "Common Issues",
      "/docs/rook/v1.7/ceph-common-issues.html",
      true,
      false
    );
  
    add(
      "CSI Common Issues",
      "/docs/rook/v1.7/ceph-csi-troubleshooting.html",
      true,
      false
    );
  
    add(
      "Monitor Health",
      "/docs/rook/v1.7/ceph-mon-health.html",
      true,
      false
    );
  
    add(
      "OSD Management",
      "/docs/rook/v1.7/ceph-osd-mgmt.html",
      true,
      false
    );
  
    add(
      "Direct Tools",
      "/docs/rook/v1.7/direct-tools.html",
      true,
      false
    );
  
    add(
      "Advanced Configuration",
      "/docs/rook/v1.7/ceph-advanced-configuration.html",
      true,
      false
    );
  
    add(
      "OpenShift Common Issues",
      "/docs/rook/v1.7/ceph-openshift-issues.html",
      true,
      false
    );
  
    add(
      "Disaster Recovery",
      "/docs/rook/v1.7/ceph-disaster-recovery.html",
      true,
      false
    );
  
    add(
      "Flex Migration",
      "/docs/rook/v1.7/flex-to-csi-migration.html",
      true,
      false
    );
  
    add(
      "Contributing",
      "/docs/rook/v1.7/development-flow.html",
      false,
      false
    );
  
    add(
      "Storage Providers",
      "/docs/rook/v1.7/storage-providers.html",
      true,
      false
    );
  
    add(
      "Multi-Node Test Environment",
      "/docs/rook/v1.7/development-environment.html",
      true,
      false
    );
  

  function getEntry(item) {
    var itemDom = document.createElement('li');

    if (item.current) {
      itemDom.innerHTML = item.name;
      itemDom.classList.add('current');
    } else {
      itemDom.innerHTML = '<a href="' + item.url + '">' + item.name + '</a>';
    }

    return itemDom;
  }

  // Flush css changes as explained in: https://stackoverflow.com/a/34726346
  // and more completely: https://stackoverflow.com/a/6956049
  function flushCss(element) {
    element.offsetHeight;
  }

  function addArrow(itemDom) {
    var MAIN_ITEM_HEIGHT = 24;
    var BOTTOM_PADDING = 20;
    var arrowDom = document.createElement('a');
    arrowDom.classList.add('arrow');
    arrowDom.innerHTML = '<img src="' + BASE_PATH + '/images/arrow.svg" />';
    arrowDom.onclick = function(itemDom) {
      return function () {
        // Calculated full height of the opened list
        var fullHeight = MAIN_ITEM_HEIGHT + BOTTOM_PADDING + itemDom.lastChild.clientHeight + 'px';

        itemDom.classList.toggle('open');

        if (itemDom.classList.contains('open')) {
          itemDom.style.height = fullHeight;
        } else {
          // If the list height is auto we have to set it to fullHeight
          // without tranistion before we shrink it to collapsed height
          if (itemDom.style.height === 'auto') {
            itemDom.style.transition = 'none';
            itemDom.style.height = fullHeight;
            flushCss(itemDom);
            itemDom.style.transition = '';
          }
          itemDom.style.height = MAIN_ITEM_HEIGHT + 'px';
        }

        return false;
      };
    }(itemDom);
    itemDom.appendChild(arrowDom);

    if ((item.current && item.children) || item.childCurrent) {
      itemDom.classList.add('open');
      itemDom.style.height = 'auto';
    } else {
      itemDom.style.height = MAIN_ITEM_HEIGHT + 'px';
    }
  }

  var menuDom = document.getElementById('docs-ul');
  for (var i = 0; i < menu.length; i++) {
    var item = menu[i];
    var itemDom = getEntry(item);

    if (item.childCurrent) {
      itemDom.classList.add('childCurrent');
    }

    if (item.children) {
      addArrow(itemDom);
      itemDom.classList.add('children');
      var children = document.createElement('ul');
      for (var j = 0; j < item.children.length; j++) {
        children.appendChild(getEntry(item.children[j]));
      }
      itemDom.appendChild(children);
    }
    menuDom.appendChild(itemDom);
  }
</script>
</div></main>
    <footer id="footer" aria-label="Footer">
  <div class="top">
    <a href="//www.cncf.io">
      <img
        class="cncf"
        src="/images/cncf.png"
        srcset="/images/cncf@2x.png 2x, /images/cncf@3x.png 3x" />
    </a>
    <p>We are a Cloud Native Computing Foundation graduated project.</p>
  </div>
  <div class="middle">
    <div class="grid-center">
      <div class="col_sm-12">
        <span>Getting Started</span>
        <a href="//github.com/rook/rook">GitHub</a>
        <a href="/docs/rook/v1.9/">Documentation</a>
        <a href="//github.com/rook/rook/blob/master/CONTRIBUTING.md#how-to-contribute">How to Contribute</a>
      </div>
      <div class="col_sm-12">
        <span>Community</span>
        <a href="//slack.rook.io/">Slack</a>
        <a href="//twitter.com/rook_io">Twitter</a>
        <a href="//groups.google.com/forum/#!forum/rook-dev">Forum</a>
        <a href="//blog.rook.io/">Blog</a>
      </div>
      <div class="col_sm-12">
        <span>Contact</span>
        <a href="mailto:cncf-rook-info@lists.cncf.io">Email</a>
        <a href="//github.com/rook/rook/issues">Feature request</a>
      </div>
      <div class="col_sm-12">
        <span>Top Contributors</span>
        <a href="//cloudical.io/">Cloudical</a>
        <a href="//cybozu.com">Cybozu, Inc</a>
        <a href="//www.redhat.com">Red Hat</a>
        <a href="//www.suse.com/">SUSE</a>
        <a href="//upbound.io">Upbound</a>
      </div>
    </div>
  </div>
  <div class="bottom">
    <div class="grid-center">
      <div class="col-8">
        <a class="logo" href="/">
          <img src="/images/rook-logo-small.svg" alt="rook.io" />
        </a>
        <p>
          &#169; Rook Authors 2022. Documentation distributed under
          <a href="https://creativecommons.org/licenses/by/4.0">CC-BY-4.0</a>.
        </p>
        <p>
          &#169; 2022 The Linux Foundation. All rights reserved. The Linux Foundation has
          registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our
          <a href="https://www.linuxfoundation.org/trademark-usage/">Trademark Usage</a> page.
        </p>
      </div>
    </div>
  </div>
</footer>


  <script src="/js/anchor.js"></script>
  <script>
    anchors.options = {
      placement: 'right',
      icon: '#',
    }

    document.addEventListener('DOMContentLoaded', function(event) {
      anchors.add('.docs-text h1, .docs-text h2, .docs-text h3, .docs-text h4, .docs-text h5, .docs-text h6');
    });
  </script>




    
  </body>
</html>
